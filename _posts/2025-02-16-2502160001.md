---
title: "[혼공머신] 5주차 06-3 주성분 분석"
date: 2025-02-16 23:24:12 +0900
categories: [대외활동, 혼공학습단]
tags: [extracurricular, hongong, machinelearning, python]
use_math: true
---
> 본 게시글은 한빛미디어의 혼자 공부하는 머신러닝+딥러닝을 바탕으로 작성되었습니다.
{: .prompt-info }

### 차원과 차원 축소
지금까지 우리는 다양한 data를 살펴보았다. Data에는 feature들이 있는데, 예를 들어 회원 data에는 이름, 나이, 성별 등 다양한 feature들이 존재한다. 그런데 한두개라면 상관 없겠으나 지난번에 살펴본 이미지의 경우 feature가 엄청나게 많이 존재한다. 예를 들어 100*100 픽셀의 이미지가 있을 경우 총 10,000개의 픽셀이 존재한다. 이러한 경우, 10,000개의 feature가 존재하게 되는 것이다. 하나의 sample에 10,000개의 feature가 있고, 수많은 data가 존재할 경우, 그 크기는 엄청나게 빠른 속도로 증가할 것이다. 또한, overfitting의 우려도 존재하며, 이를 방지하기 위한 더 많은 data는 더 많은 시간과 돈이 소요된다. 보통 이러한 경우 더 많은 data를 구할 수도 없고 우리는 시간과 돈 둘 다 없기에 크기를 줄일 수 있는 방법을 찾아야 한다.

그런데, 단순히 data 사이즈만 줄이는 경우 underfitting이 발생할 수 있다. 그러면 어떻게 해야 할까? 이를 위해 차원 축소(Dimensionality Reduction)라는 방법이 존재한다. Dimensionality reduction은 data를 가장 잘 나타낼 수 있는 특성만을 선택하는 방법이다. 이번에는 dimensionality reduction의 대표적인 방법인 주성분 분석(PCA, Principal Component Analysis)를 살펴볼 것이다.

### 주성분 분석
PCA는 data 내의 variance이 가장 큰 방향을 찾는 것이라고 생각하면 된다. 예를 들어 아래와 같이 한 방향으로 길게 분포하고 있는 data가 존재한다고 하자.
![image](https://www.dropbox.com/scl/fi/7tkwo8tzg7mvqbj2dhnzj/6.jpg?rlkey=tcl56r2h9ron8ms9ee6vqdewb&st=s5fnu0tr&raw=1)

이때, 길게 뻗은 data 방향으로 선을 그을 수 있을 것이다. 이렇게 길게 뻗은 data 방향으로 그은 선을 우리는 주성분(Principal Component)이라고 한다. 
![image](https://www.dropbox.com/scl/fi/va9dqzd4s2xw6eo1vgw9m/7.jpg?rlkey=0tpvqkqfheijsy8pnv88s4cb1&st=t74dnx2a&raw=1)

2차원 좌표평면 위에 있는 data들을 이 선 위로 투영시킨다면 모든 data들이 1차원 점이 될 수 있을 것이다. 
![image](https://www.dropbox.com/scl/fi/igj2of6xgcgix6rdqqafb/8.jpg?rlkey=4wmn0urs0yc6hqf5839pihkjz&st=2v3tecn5&raw=1)

만약 첫번째 principal component을 찾은 후에, 이 선, 즉 벡터에 수죽이고 분산이 가장 큰 방향을 찾아 2번째 principal component를 찾는다. 방금 살펴본 것과 같이 일반적으로 principal component은 feature의 개수만큼 찾을 수 있다. 이제 파이썬으로 찾아보자.

### PCA
지난번과 마찬가지로 과일 이미지를 가져오고 동일하게 과일 그림을 그리는 함수를 만들어서 이미지를 살펴보자. PCA를 실행하니 아래와 같이 정체를 알 수 없는 그림이 나왔다. 아래의 그림은 가장 variance이 큰 방향으로 principal component를 순서대로 구해서 나타낸 것이다. 이렇게 principal component를 구했으므로, 기존의 10,000개의 feature를 방금 찾은 50개릐 principal component의 feature로 줄여볼 수 있다. transform을 통해서 원본 데이터의 dimension을 줄여보도록 하자.
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

data = np.load('fruits_300.npy')
data_2d = data.reshape(-1, 100*100)

pca = PCA(n_components=50)
pca.fit(data_2d)

import matplotlib.pyplot as plt
def draw_fruit(arr, ratio=1):
    n = len(arr)
    rows = int(np.ceil(n/10))
    cols = n if rows < 2 else 10
    fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False)
    for i in range(rows):
        for j in range(cols):
            if i * 10 + j < n:
                axs[i, j].imshow(arr[i*10+j], cmap='gray_r')
            axs[i, j].axis('off')
    plt.show()

draw_fruit(pca.components_.reshape(-1, 100, 100))
data_pca = pca.transform(data_2d)
```
![image](https://www.dropbox.com/scl/fi/9exmfmve2n5ukzzxn7sy1/1.jpg?rlkey=a7p3mgwk4zl1igg5ej7hsyrw4&st=a9hygsg3&raw=1)

그런데 한 가지 궁금한 것이 있다. 만약 dimension을 줄인 다음 원래대로 복원은 불가능할까? 답을 먼저 말하자면 어느 정도 가능하다. 아래의 이미지를 보면, 기존의  과일들을 거의 완벽하게 복원한 것을 확인할 수 있다. 즉, 아까 만든 50개의 principal component가 기존의 data를 매우 잘 설명하고 있는 것이다. 그러면 얼마나 principal component가 기존의 data를 설명하는지 알고 싶을 것이다.
```python
data_inverse = pca.inverse_transform(data_pca)

fruit_reconstruct = data_inverse.reshape(-1, 100, 100)
for start in [0, 100, 200]:
    draw_fruit(fruit_reconstruct[start:start+100])
    print('\n')
```
![image](https://www.dropbox.com/scl/fi/equ41trfen220w0rnep5f/2.jpg?rlkey=lxyi48poqyc9hjtn4ndmiryh0&st=krnbg6v4&raw=1)
![image](https://www.dropbox.com/scl/fi/d8kq4ahc3z8l2vjs9l3or/3.jpg?rlkey=6asq3pgzwh2ugnk7wz1sbhgvs&st=1gx2fdff&raw=1)
![image](https://www.dropbox.com/scl/fi/s9iucvo9ke8lkz257s3sz/4.jpg?rlkey=1y3vi21wc9lkqaoqgtksduxxs&st=jsfdno3y&raw=1)

### 설명된 분산
방금 말한, principal component가 원본 data의 variance을 얼마나 잘 나타내는지를 기록한 것을 설명된 분산(Explained Variance)라고 한다. 아까 우리가 만든 principal component들의 순서는 얼만 이 explained variance가 큰지를 바탕으로 순서대로 구한 것이다. 그럼 이것을 값으로 한 번 살펴보도록 하자. 값을 살펴보니 무려 92%를 설명한다는 것을 확인할 수 있다.
```python
np.sum(pca.explained_variance_ratio_)
```
> 0.9214952099938081

그리고 여기서 또 한 가지 도움이 되는 것을 찾을 수 있다. 지난번에 K-Means 알고리즘을 살펴보았을 때 최적의  $K$값을 찾는 것을 해본 기억이 있을 것이다. 그때, 값이 감소하는 속도를 바탕으로 최적의 $K$값을 찾았었는데, 여기서도 동일하게 사용할 수 있다. 여기서도 최적의 principal component 개수를 explained variance의 값을 가지고 살펴보도록 하자. 아래의 plot을 살펴보면, 10 전까지는 값이 어느정도 큰데, 그 이후로는 값의 변화가 거의 없는 것을 확인할 수 있다. 따라서 처음 10개의 principal component를 사용하면 어느정도 variance를 표현할 수 있을 것이다.
```python
plt.plot(pca.explained_variance_ratio_)
plt.show()
```
![image](https://www.dropbox.com/scl/fi/zs6rr5fct4z0nvk3ulxr1/5.jpg?rlkey=ydmfkmr6j5lajp7vr3aoa5sb7&st=29ad8eja&raw=1)

### 다른 알고리즘과 사용하기
