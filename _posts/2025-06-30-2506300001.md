---
title: "[혼만딥] 1주차 합성곱 신경망 이해하기"
date: 2025-06-30 01:38:01 +0900
categories: [대외활동, 혼공학습단]
tags: [extracurricular, hongong, deeplearning, python, lenet]
use_math: true
published: false
---
> 본 게시글은 한빛미디어의 혼자 만들면서 공부하는 딥러닝을 바탕으로 작성되었습니다.
{: .prompt-info }

### 시작하기 앞서...
지난번에는 혼공학습단에서 혼자 공부하는 머신러닝+딥러닝 책을 가지고 활동을 했었는데 마무리가 조금 아쉬웠다. 그리고 엄청 간단한 내용들만 있을 줄 알고 가벼운 마음으로 시작해 많이 허술하게 한 감이 없잖아 있었다. 따라서 이번에는 새로 나온 혼자 만들면서 공부하는 딥러닝 책으로 새로운 마음으로 시작해 보려고 한다. 그리고 이번에는 또 원대한 계획을 세웠다. 기본적으로 혼공학습단은 총 6주 동안 책을 다루는데, 진도는 4장까지밖에 안 나간다. 앞부분도 충분히 의미있고, 다양한 내용이 있다. 그러나 뒷부분이 더 재미있어 보이고, 지난번 데이콘 컴페티션에서 T5 모델을 약간 다루긴 했었는데 T5 모델에 대해서 자세히 살펴보지는 않아서 뒷부분에 약간 더 흥미가 생겼다. 따라서 이번에는 하루에 한 장씩 끝내고, 최종적으로 한 권을 다 끝내는 것을 목표로 하기로 했다.
물론 

그동안 주로 LaTeX을 중심으로 쓰다가 다시 Markdown으로 돌아오니까 조금 불편한 것이 많다. 처음에는 Markdown이 따로 설정할 필요 없이 텍스트를 적으면 되어서 편했는데 LaTeX의 여러 패키지를 쓰다보니 이젠 패키지가 없는 것이 더 아쉬운 느낌이다. Markdown에서 수식을 쓰는 것도 너무 귀찮다. Jekyll 블로그에서 LaTeX 수식 뷰어랑 VS Code에서 사용하는 수식 뷰어가 내가 기억하기로는 달랐던 것으로 기억한다. 그래서 VS Code에서 수식이 이쁘게 보이면 Jekyll 블로그나 Github 뷰어로는 수식이 깨지고 그 반대로 하면 VS Code에서 수식이 안 보였던 것으로 기억한다. 그래서 PDF로 출력이 되던 LaTeX 문서 작성이 더 쓰고 싶어진다. 하지만 블로그를 여기서 쓰기로 한 이상 이건 어쩔 수 없긴 하다.

어쨌든 다시 혼공학습단을 시작해보자. 지난번에는 가벼운 마음으로 시작했다면 이번에는 엄청나게 부담을 갖고 빡세게 해볼 것이다.

### 인공 신경망
합성곱 신경망의 초기 형태, 필기 숫자 데이터인 MNIST 데이터를 인식하기 위해 만들었음
다중 분류 문제를 해결하는 것




#### 입력층
데이터를 입력받는 곳, 신경망이 처리해야할 원본 데이터를 전달하는 역할

#### 은닉층

#### 출력층
최종 결과를 만들어내는 곳

### 합성곱 신경망

#### LeNet

#### 합성곱층

#### 풀링층







#### 오차 역전파법
unit $j$에 대하여 모든 input data $x_j$는 $j$와 연결된 unit $i$들의 output $y_i$와 weight $w_{ij}$의 곱들의 합으로, 다음과 같이 정의할 수 있다.
$$x_j=\sum_{i}y_iw_{ji}\tag{1}$$
이때, bias를 처리하기 위해 각 unit에 항상 1의 값을 갖는 추가 입력을 준다.

unit $j$의 출력 $y_j$는 모든 input data $x_j$에 대한 non-linear function $\phi$로, 미분가능한 함수여야 한다.
$$y_j=\frac{1}{1+e^{-x_j}}$$

목표는 각 입력 데이터에 대해 네트워크가 생성하는 output vector가 원하는 output vector와 동일하거나 가까워지게 하는 가중치 집합을 찾는 것이다.
input-output의 집합이 유한하다면 특정 가중치 집합에서 네트워크의 total error는 각 사례에 대한 output과 목표 output 간의 가치를 비교함으로써 알 수 있다.

$$E=\sum_{c}\sum_j\frac{1}{2}(y_j-d_j)^2$$

여기서 오차 $E$를 gradient descent로 최소화하기 위해서는 각 weight에 대한 error의 편미분을 계산해야 한다. 단순히 각 사례에 대한 편미분의 합을 구하면 된다.

오차의 편미분은 순방향 전차와 역방향 전파 총 두 단계에서 계산된다.

특정 사례 $c$에 대해서
$$\frac{\partial E}{\partial y_j}=\frac{\partial}{\partial y_j}\sum_{c}\sum_j\frac{1}{2}(y_j-d_j)^2$$

$$\frac{\partial}{\partial y_j}\sum_{c}\sum_j\frac{1}{2}(y_j-d_j)^2=\frac{\partial}{\partial y_j}\sum_j\frac{1}{2}(y_j-d_j)^2=\frac{\partial}{\partial y_j}\frac{1}{2}(y_j-d_j)^2$$

$$\frac{\partial}{\partial y_j}\frac{1}{2}(y_j-d_j)^2=\frac{\partial}{\partial y_j}\frac{1}{2}(y_j^2-2y_jd_j+d_j^2)$$

$$\frac{\partial}{\partial y_j}(\frac{1}{2}y_j^2-y_jd_j+\frac{1}{2}d_j^2)=y_j-d_j$$

$$\frac{\partial E}{\partial x_j}=\frac{\partial}{\partial y_j}\sum_{c}\sum_j\frac{1}{2}(y_j-d_j)^2$$

$$\frac{\partial}{\partial x_j}\sum_j\frac{1}{2}(y_j-d_j)^2=\frac{\partial}{\partial x_j}\frac{1}{2}(y_k-d_k)^2$$

$$\frac{\partial E}{\partial x_j}=\frac{\partial E}{\partial y_j}\frac{\partial y_j}{\partial x_j}$$

$$\frac{dy_j}{dx_j}=\frac{d}{dx_j}\frac{1}{1+e^{-x_j}}$$

$$f(x)=1+e^{-x_j}$$

$$f'(x)=\frac{d}{dx_j}(1+e^{-x_j})=-e^{-x_j}$$

$$\frac{d}{dx}\left(\frac{1}{f(x)}\right)=-\frac{f'(x)}{(f(x))^2}$$

$$\frac{d}{dx_j}\left(\frac{1}{1+e^{-x_j}}\right)=-\frac{-e^{x_j}}{(1+e^{-x_j})^2}=\frac{e^{-x_j}}{(1+e^{x_j})^2}$$

#### 밀집층


























### 번외

#### 모델 시각화

### 기본 과제
#### 구글 코렙에 tensorflow/keras 설치하기

#### LeNet 모델 시각화하기

### 추가 과제
#### Convolutional Layer 설명하기

#### Pooling Layer 설명하기

#### Dense Layer 설명하기

### 참고문헌
[1] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proc. IEEE, vol. 86, no. 11, pp. 2278–2324, Nov. 1998, doi: 10.1109/5.726791.  
[2] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations by backpropagating errors,” 1986.  
[3] W. S. Mcculloch and W. Pitts, “A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY”.  
[4] K. Fukushima, “Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,” Biol. Cybernetics, vol. 36, no. 4, pp. 193–202, Apr. 1980, doi: 10.1007/BF00344251.  
[5] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification with deep convolutional neural networks,” Commun. ACM, vol. 60, no. 6, pp. 84–90, May 2017, doi: 10.1145/3065386.
