---
title: "[혼공머신] 2주차 03-3 특성 공학과 규제"
date: 2025-01-17 16:17:55 +0900
categories: [대외활동, 혼공학습단]
tags: [extracurricular, hongong, machinelearning, python]
use_math: true
---
> 본 게시글은 한빛미디어의 혼자 공부하는 머신러닝+딥러닝을 바탕으로 작성되었습니다.
{: .prompt-info }

### 다중 회귀
지난번에 살펴본 linear regression model은 조금 더 세부적으로 분류해본다면 simple linear regression model(SLR)이라고 말을 했을 것이다. 이것은 하나의 independent variable이 있다는 것이다. 하지만 우리가 살고 있는 세상은 그렇게 쉽게 다룰 수 있는 대상이 아니다. 예를 들어서 학교 성적을 predict한다고 해보자. 그럼 우리는 independent variable로 공부 시간, 공부 장소, 성격, 학군, 심지어 날씨, 기분 이런 것도 포함해서 분석할 수 있을 것이다. 우리는 이렇게 하나의 variable만 사용하는 것이 아니라, 여러 개의 variable, feature를 사용하는 경우의 회귀를 다중 회귀(Multiple Regression)라고 한다.

그런데 꼭 multiple regression를 써야 하는 것일까? 그냥 단순회귀만 쓰면 안 될까? 라는 귀차니즘이 생길 수 있을 것이다. 그렇다면 아래의 그래프를 살펴보자. 우선 그래프는 x, y, z 세 개의 variable이 있다. 그런데 x, y만 가지고 봤을 때는 어떤 관계가 보이는가? 물론 하나로 잘 뭉쳐있다고 할 수는 있다. 하지만 linear 관계는 찾아보기 힘들 것이다.

![image](https://www.dropbox.com/scl/fi/emel4jkcttrgvfqsnhxe6/2501170003-1.jpg?rlkey=q664zck53dqzl3fwgkdn8n0ju&st=i5ojvxny&raw=1)

이제 다른 것을 가지고 살펴보자. y와 z를 가지고 살펴봤을 때는 조금 뭔가 linear한 관계가 보이는 것 같기도 하지만 그래도 조금 아쉽다.

![image](https://www.dropbox.com/scl/fi/kwf74gp9cbqbzuhsnpyn9/2501170003-2.jpg?rlkey=mk310kw7ogpspd2piu9o9i8lo&st=gls0ay80&raw=1)

그런데 이제 3개의 variable을 모두 사용하여 이리저리 기울이다 보면 전혀 다른 결과가 나온다. 아래의 그래프를 보자. 완벽한 linear한 관계를 보인다.

![image](https://www.dropbox.com/scl/fi/f2iphoyqu7lknwmitfrm1/2501170003-3.jpg?rlkey=jsh7cknpbbm8a44nynqc54iir&st=vt3egb4w&raw=1)

이와 같이 일부의 데이터로는 관계를 파악할 수 없으나, 다른 데이터가 추가되면 관계가 나타나는 경우가 있다. 이러한 경우를 위해 우리는 multiple regression를 사용해야 하는 것이다. 혹시라도 이러한 그래프가 실제로 존재하는 것인지 의문을 품을 수 있으므로 아래에 예쁘게 첨부하였다. 마우스로 이리저리 움직이면서 데이터를 살펴보도록 하자.

<div style="text-align: center;">
  <iframe src="/assets/html/2501170003-1.html" width="600" height="400"></iframe>
</div>

### 특성 공학

그런데 우리는 이렇게 variable이 3개인 경우 visualization을 통해서 관계를 확인할 수 있다. 그러면 만약 4개인 경우에는 어떻게 할까? 아래 도형처럼 어떻게든 4차원을 생각해서 구현한 다음에 분석을 해야할까? 그건 말이 안 된다. 대신 다른 방법을 사용할 수 있다. feature를 서로 곱하거나 더하는 등 서로 결합하여 새로운 feature를 만들어 개별 feature의 수를 줄이는 것이다. 예를 들어 지난번에 살펴본 농어의 데이터를 생각하자면 농어의 길이와 높이를 곱한 크기라는 새로운 feature를 만들어서 처리하는 것이다. 이렇게 기존의 feature를 활용해 새로운 feature를 만들어내는 것을 특성 공학(Feature Engineering)이라고 한다.

![image](https://upload.wikimedia.org/wikipedia/commons/5/55/8-cell-simple.gif)

[이미지 출처: [위키피디아 Four-dimensional space 문서](https://en.wikipedia.org/wiki/Four-dimensional_space)]

이제 농어의 데이터를 활용하여 이 내용을 살펴보자. 이번에는 농어의 길이, 너비, 높이 데이터를 가지고 볼 것이다. 우선 데이터를 입력하자. 교재에서는 read_csv로 파일을 다운로드 받아서 실행하기는 했지만, 인터넷에서 불러오는 방식은 시간이 지나면 자료 손실의 우려가 있기에 나는 저장하거나, 직접 기록하는 것을 선호한다. 그렇기에 친절히 다 적어두었다. 당연히 내가 손으로 다 적은 것은 아니고 ChatGPT에게 시켰다. 이후 우리는 앞으로 계속 보게 될 데이터프레임(Dataframe)으로 변환하였다. Dataframe은 Numpy의 array와 비슷하다고 생각할 수 있다. 하지만 더 많은 기능을 제공하며, 일종의 엑셀 스프레드시트처럼 볼 수 있다고 생각하면 된다. 참고로 Dataframe은 Pandas 패키지를 통해 사용할 수 있다.

```python
import numpy as np
import pandas as pd

length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 
       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 
       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 
       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 
       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 
       44.0])

height = np.array([2.11, 3.53, 3.82, 4.59, 4.59, 5.22, 5.2, 5.64, 5.14, 5.08, 5.69, 
       5.92, 5.69, 6.38, 6.11, 5.64, 6.11, 5.88, 5.52, 5.86, 6.79, 5.95, 
       5.22, 6.28, 7.29, 6.38, 6.73, 6.44, 6.56, 7.17, 8.32, 7.17, 7.05, 
       7.28, 7.82, 7.59, 7.62, 10.03, 10.26, 11.49, 10.88, 10.61, 10.84, 
       10.57, 11.14, 11.14, 12.43, 11.93, 11.73, 12.38, 11.14, 12.8, 11.93, 
       12.51, 12.6, 12.49])

width = np.array([1.41, 2.0, 2.43, 2.63, 2.94, 3.32, 3.12, 3.05, 3.04, 2.77, 3.56, 
       3.31, 3.67, 3.53, 3.41, 3.52, 3.52, 3.52, 4.0, 3.62, 3.62, 3.63, 
       3.63, 3.72, 3.72, 3.82, 4.17, 3.68, 4.24, 4.14, 5.14, 4.34, 4.34, 
       4.57, 4.2, 4.64, 4.77, 6.02, 6.39, 7.8, 6.86, 6.74, 6.26, 6.37, 
       7.49, 6.0, 7.35, 7.11, 7.22, 7.46, 6.63, 6.87, 7.28, 7.42, 8.14, 
       7.6])

weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])

data = {
    'length': length,
    'height': height,
    'width': width
}

df = pd.DataFrame(data)
```

이제 dataframe을 만들었고, 지난번에 했던 것처럼 array로 바꾸고, train data와 test data로 나눠주자. 지금부터는 random_state를 42로 적어야겠다. 절대 내가 반골 기질로 0으로 바꿔서 썼지만 일일이 바꾸기 귀찮아져서는 아니다. 42를 많이 쓰는 것도 그냥 관례적으로 많이 쓰는 것이기에 내가 그 관계를 굳이 반해서 얻을 것도 없기 때문이다.

```
from sklearn.model_selection import train_test_split

perch_full = df.to_numpy()
X_train, X_test, y_train, y_test = train_test_split(perch_full, perch_weight, random_state=42)
```

### 변환기
이제 우리가 아까 말했던 feature engineering을 할 것이다. Scikit-learn에서는 특성 전처리를 위한 메서드가 있는데, 이것을 변환기(Transformer)라고 한다. 