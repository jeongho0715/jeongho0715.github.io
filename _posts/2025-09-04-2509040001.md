---
title: "[논문 리뷰] Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models"
date: 2025-09-04 23:43:36 +0900
categories: [논문 리뷰, 딥러닝]
tags: [deeplearning, llm, bias, fair, prompt]
use_math: true
published: true
---

# 논문 정리
## 서론
- LLM은 train 데이터에 있는 편향을 지속하고 재생산함.
- LLM의 편향 완화 위해 모델 표현을 재학습하거나 추가적인 데이터로 fine-tuning 하는 방법 있음.
- 하지만 최신 LLM은 대부분 내부 구조나 학습 데이터를 수정하기 어려운 폐쇄형 API 전용 모델임.
- 오픈소스 LLM을 사용하더라도 충분한 데이터를 수집하는 것은 어려움.
- 모델이나 출력 확률에 접근하지 않고, 즉 프롬프트 기반으로 LLM의 편향을 해결할 수 있을까?

## 선행 연구
- LLM의 train 데이터는 매우 방대하기에 편향되거나 해로운 데이터를 검증하는 것은 거의 불가능함.
- 최근 연구는 사후적 편향 완화 기법에 집중하기도 했으나, 재학습, 파라미터 접근 등이 필요함.
- 일반적인 프롬프트 엔지니어링 방식은 모델에 지시문을 제공하고 텍스트를 생성하게 하는 것이나 추론 작업에는 한계 존재함.
- Kong et al.: 역할, 페르소나 부여, zero-shot 성능 개선.
- Brown et al.: Few-shot으로 예시를 입력해 모델이 학습하게 함.
- 모델에 중간 추론 단계를 제공하는 Chain-of-Thought를 제공하기도 함.
- LLM이 출력을 생성하고 피드백을 제공받은 후 self-refine하는 방법도 제시됨.
- 프롬프트 엔지니어링 방법은 연구되고 있으나 공정한 텍스트를 생성하기 위한 것은 충분히 연구되지 않음.
- 기존 프롬프트 엔지니어링 연구는 편향 완화에 효과가 미비하다고 보고했으나, 성별 편향 완화 적용 사례 존재.

## 프롬프트 엔지니어링
### Prefix Prompting
- 단순히 입력 프롬프트 앞에 접두어를 추가해 모델에게 "편향되지 말라"고 지시하기.
- 기존의 사용자 입력 프롬프트 $C$가 있을 때 앞에 편향을 완화하게 하는 지시인 $I_{debias}$를 결합함.

$$C_{debias}=concat(I_{debias}, C)$$

### Self-Refinement
- 단순히 prefix prompting으로는 부족함.
- k-step 접근으로 모델이 이전 단계의 출력을 참고해 다음 출력을 생성하도록 함.

#### 1단계
- 사용자 입력 프롬프트 $C$가 있을 때, 편향 완화 접두어 $I_{debias}$를 넣어 $C_{debias}$를 입력으로 사용함.
- 이를 통해 편향이 있을 수 있는 텍스트 $S_0$가 생성됨.

$$C_{debias}=concat(I_{debias}, C)$$

#### 2단계
- 1단계에서 출력 $S_0$에 다른 편향 완화 접두어 $I_{SR}$을 추가해 최종 입력 프롬프트 $C_{SR}$을 만듦.
- 최종 입력 프롬프트 $C_{SR}$을 통해 최종적으로 편향이 완화된 텍스트 $S_1$을 얻음.

$$C_{SR}=concat(I_{SR}, S_0)$$

### Implication Prompting
- Self-Refinement 방식은 LLM이 스스로 생성한 출력을 바탕으로 편향되지 않은 텍스트를 만들어야 하기에 부족할 수 있음.
- 이를 위해 LLM이 이전의 출력에 존재하는 편향을 스스로 식별하게 함.

#### 1단계
- 사용자 입력 프롬프트 $C$가 있고, 이를 통해 편향이 있을 수 있는 텍스트 $S$가 생성됨.

#### 2단계
- 모델에게 출력에 존재하는 편향(함의)를 찾도록 하는 지시문인 $I_{Impl}$를 설정함.
- 1단계의 출력인 $S$와 두번째 지시문인 $I_{Impl}$을 결합한 $C_{Impl}$을 입력으로 사용함.
- 결과적으로 1단계의 출력인 $S$에 어떤 편향이 있는지 출력함.

#### 3단계
- 모델에게 편향을 완화하도록 하는 지시문인 $I_{IP}$를 설정함.
- 초기의 편향의 우려가 있는 $S$와 2단계에서 $S$에 존재하는 편향을 식별한 결과인 $S_{Impl}$, 그리고 $I_{IP}$를 결합함.
- 최종적으로 1단계의 출력 $S$와 그 안에 존재하는 편향인 $S_{Impl}$을 바탕으로 편향되지 않게 출력함.

## 모델 및 실험 환경
- 본 연구에서는 4가지 LLM(GPT-J(6B), Mistral-v0.1(7B), LIama-2(7B), MPT-Instruct(7B))을 사용함.
- 32GB NVIDIA V100 GPU 1개에서 수행.

## 데이터셋
- 성별, 인종, 종교, 직업에 대한 편향 측정 위한 데이터셋.
- 각 문장은 빈칸이 있는데, 이 빈칸을 고정관념이 있거나, 고정관념에 반대되거나 전혀 무관한 단어로 채울 수 있음.
- 예) 한국인은 _____다. 1) 시끄럽, 2) 조용하, 3) 사과

## 평가지표
### StereoSet
- 성별, 인종, 종교, 직업에 대한 편향 측정 위한 데이터셋.
- 각 문장은 빈칸이 있는데, 이 빈칸을 고정관념이 있거나, 고정관념에 반대되거나 전혀 무관한 단어로 채울 수 있음.
- 예) 한국인은 _____다. 1) 시끄럽, 2) 조용하, 3) 사과
#### Stereotype Score(SS)
- 고정관념이 있는 문장이 고정관념에 반대되는 문장보다 높은 확률을 가지는 비율.
- 50%가 이상적 수치.

#### Language Modeling Score(LM)
- 무관한 문장이 가장 낮은 확률을 가질 비율.
- 100%가 이상적 수치.

#### Idealized Context Association Test Score(ICAT)
- SS와 LM을 결합해 균형을 나타냄.
- 100%가 이상적 수치.

### Regard
- 이전에는 감정 분석 분류기를 많이 썼으나, 감정과 편향이 반드시 연결되어 있지 않다는 한계 극복하기 위한 데이터셋.
- 성별, 인종,성적 지향에서의 편향 포착
- 각 인구 집단마다 10개의 프롬프트 템플릿을 구성하고, 각 프롬프트 템플릿마다 10개의 문장 생성.
- 분류기를 사용하여 각 집단의 출력별 regard를 계산.
- 예를 들어 남성의 Regard 점수는 다음과 같이 긍정으로 평가된 응답의 개수에 부정으로 평가된 응답의 개수를 빼고, 전체 응답의 개수로 나누어 구함.

$S_{Male}=\frac{N_{pos}-N_{neg}}{N_{total}}$

- 성별에 대한 최종 Regard 점수를 구한다면, 각 집단, 남성과 여성의 Regard 점수 차이로 구할 수 있음.

$R_{Gender}=S_{Female}-S_{Male}$

- 0이 이상적 수치, 음수면 고정관념 편향, 양수는 고정관념에 반대되는 편향을 의미함.

### Toxicity
- 모델의 독성 완화 능력 평가 지표.
- 무작위로 샘플링한 1,000개의 프롬프트에 대해 출력에 독성이 있을 확률 계산.
- 평균 독성 점수는 낮을 수록 좋으며, 기본 모델 대비 독성 변화율은 음수일수록 좋음.

## 결과
- 모델에게 역할, 페르소나를 부여하는 prefix prompt가 단순한 지시문인 prefix prompt보다 3가지 지표 모두에서 더 뛰어났음.
- 이전 출력을 prefix prompt와 결합할 경우 toxicity를 제외하고 유의미하게 성능 향상됨.
- Self-Refinement를 한 번 했을 경우, 성능 개선이 의미있었으나 2회 이상인 경우 성능 개선 미비함.
- 함의 프롬프트는 모든 방법 중 가장 성능이 뛰어났음.
- 기존 연구에서는 출력 편향이 줄어들수록 언어 모델링 능력이 감소했으나, 여기서는 일관된 결과 없었음.

## 향후 연구
- 계산 자원 제약으로 대규모 모델이나 Mixture of Experts 모델 실험 못함.
- 시간적 제약으로 Tree-of-Thought, Self-Consistency, Directional Stimulus Prompting과 같은 발전된 프롬프트 기법 탐구 못함.
- 복잡한 사회적 편향을 단순화해 언어 모델 내 모든 편향을 포착하지 못할 수 있음.
- 프롬프트에 의존하기에 LLM 모델이 다르거나 업데이트될 경우 결과가 달라질 수 있음.

# 참고문헌
Furniturewala, S., Jandial, S., Java, A., Banerjee, P., Shahid, S., Bhatia, S., & Jaidka, K. (2024). Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models (No. arXiv:2405.10431). arXiv. https://doi.org/10.48550/arXiv.2405.10431
