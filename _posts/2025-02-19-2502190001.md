---
title: "[혼공머신] 6주차 07-1 인공 신경망"
date: 2025-02-19 10:39:49 +0900
categories: [대외활동, 혼공학습단]
tags: [extracurricular, hongong, machinelearning, python]
use_math: true
published: false
---
> 본 게시글은 한빛미디어의 혼자 공부하는 머신러닝+딥러닝을 바탕으로 작성되었습니다.
{: .prompt-info }
### 패션 MNIST
패션과 관련된 데이터인 fashion MNIST 데이터셋을 사용해서 classification을 해볼 것이다. 먼저 tensorflow의 keras 패키지에 있는 fashion MNIST data를 불러온다. Data의 모양을 살펴보니, 60,000개의 sample이 있고, 그 sample 이미지의 가로 세로 크기는 28이다. 그리고 target을 살펴보니 target은 60,000개가 있다.
```python
from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

print(train_input.shape, train_target.shape)
```
> (60000, 28, 28) (60000,)

지난번에 사과 그림을 그렸던 것처럼 이번에는 fashion mnist를 시각화해보자. 아래와 같이 다양한 패션과 관련된 아이템들이 나오는 것을 확인할 수 있다. 그런데 보면 약간 비슷해 보이는 요소들이 보인다. 그런데 이것이 input data이므로 output, 즉 target data는 아마 이것을 분류하는 것일 것이다.
```python
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 10, figsize=(10,10))
for i in range(10):
    axs[i].imshow(train_input[i], cmap='gray_r')
    axs[i].axis('off')
plt.show()
```
![image](https://www.dropbox.com/scl/fi/qsh4d176838q56qfiassk/1.jpg?rlkey=25w33nmbv0upluw11sbcahwry&st=on4s4g1b&raw=1)

이제 target data를 살펴보기로 하자. 방금 살펴보았던 data들의 target data를 살펴보니 아래와 같았다. 하지만 이것으로만 어떻게 classify되었는지 확인하기는 조금 어려울 수도 있어 보인다. 한번 이 항목들의 의미를 살펴보도록 하자.
```python
print([train_target[i] for i in range(10)])
```
>[9, 0, 0, 3, 0, 2, 7, 2, 5, 5]

한빛미디어는 아래와 같이 우리의 label이 어떤 의미인지 알려준다. 
|label|0|1|2|3|4|5|6|7|8|9|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|item|티셔츠|바지|스웨터|드레스|코트|샌달|셔츠|스니커즈|가방|앵클 부츠|

### 로지스틱 회귀
이렇게 fashion mnist data를 살펴보았다. 그러면 이제 classification을 실제로 해보기로 하자. 지금까지 우리는 여러가지를 배웠는데, classification 방법에 대해서 하나를 배운 것이 있다. 바로 logistic regression이다. 이전에 이미 해본 적이 있기 때문에 빠르게 한번 해보기로 하자. 다만, SGDClassifier는 2-dimension input이 불가하기에 각 sample을 1-dimension으로 만들어준 다음에 실행해보기로 하자. Cross validation까지 해보니, 평균적인 score가 0.81945가 나왔다. 나쁘지 않은 score이긴 하지만 이 정도 수준이 마음에 들지 않을 수 있다. 
```python
from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier

train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)

model = SGDClassifier(loss='log_loss', max_iter=5, random_state=42)

scores = cross_validate(model, train_scaled, train_target, n_jobs=-1)
np.mean(scores['test_score'])
```
> 0.81945

### 인공신경망
사실 혼공학습단을 하려고 했던 것은 이 인공신경망에 대해 다루기 위한 것이 제일 컸다. 그런데, 정작 인공신경망을 다룰 때가 되니 시간도 없고 거의 끝나가 엄청 자세하게 다루기는 많이 어려울 것 같아 많이 아쉽다. 나중에 기회가 된다면 자세하게 다뤄보기로 하고 여기서는 가볍게 다루기로 하자.

인공신경망의 경우, 가장 기초적인 구조를 퍼셉트론이라고 할 수 있겠는데, 이것에 대해서는 [퍼셉트론](https://jeongho0715.github.io/posts/2501240001/) 게시글에서 다뤄본 적이 있다. 따라서 이론적인 부분은 여기서 살펴보고, 아래와 같이 가중치를 통해서 값을 변경해가면서 학습을 하게 되는 것이다.

![image](https://www.dropbox.com/scl/fi/15hf1l3wx0m7ige0z5h67/perceptron.jpg?rlkey=p6oos6h59l4rr0l3hgczrr07m&st=o9bqeeov&raw=1)

### 텐서플로우
텐서플로우는 구글이 오픈소스로 공개한 딥러닝 라이브러리이다. 사실 지금 Pytorch를 공부를 해야 해서 많이 마음이 급하다. 그래도 텐서플로우에 대해서 조금이라도 다뤄보기로 하자. 아래와 같이 먼저 늘 하듯 train data와 test 데이터를 나누고, 뉴런을 10개, 소프트맥스 함수로 구성하고, 크로스 엔트로피 손실 함수를 사용해서 학습을 한다. 최종적으로 성능은 0.85가 나왔다. 아까와 비교했을 때 매우 큰 수준의 향상이라고 할 수 있다. 그런데 이 모델은 조금 간단한 모델이다. 더 추가적인 모델은 다음 게시글에서 살펴보자.

```python
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.3, random_state=42)
dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))
model = keras.Sequential([dense])
model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(train_scaled, train_target, epochs=5)
```
> ```
> Epoch 1/5
> 1313/1313 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.7299 - loss: 0.8061
> Epoch 2/5
> 1313/1313 ━━━━━━━━━━━━━━━━━━━━ 1s 972us/step - accuracy: 0.8324 - loss: 0.4951
> Epoch 3/5
> 1313/1313 ━━━━━━━━━━━━━━━━━━━━ 1s 992us/step - accuracy: 0.8444 - loss: 0.4651
> Epoch 4/5
> 1313/1313 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8494 - loss: 0.4502  
> Epoch 5/5
> 1313/1313 ━━━━━━━━━━━━━━━━━━━━ 1s 994us/step - accuracy: 0.8524 - loss: 0.4408 
> ```


```python
model.evaluate(val_scaled, val_target)
```
> ```
> 563/563 ━━━━━━━━━━━━━━━━━━━━ 1s 911us/step - accuracy: 0.8507 - loss: 0.4426
> [0.4461909234523773, 0.8516666889190674]
> ```

### 확인 문제
1. 어떤 인공 신경망의 입력 특성이 100개이고 밀집층에 있는 뉴런의 개수가 10개일 때 필요한 모델 파라미터의 개수는 몇 개인가요?
파라미터의 수는 입력 특성의 수*뉴런의 수+bias의 수로 구할 수 있다. 이런 경우, bias는 뉴런마다 1개씩 존재한다. 따라서 결과적으로 1,010개가 나옴을 알 수 있다.