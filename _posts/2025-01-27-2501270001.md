---
title: "[혼공머신] 3주차 04-2 확률적 경사 하강법"
date: 2025-01-27 12:26:09 +0900
categories: [대외활동, 혼공학습단]
tags: [extracurricular, hongong, machinelearning, python]
use_math: true
---
> 본 게시글은 한빛미디어의 혼자 공부하는 머신러닝+딥러닝을 바탕으로 작성되었습니다.
{: .prompt-info }

### 점진적 학습
우리는 지금까지 한정된 data를 가지고 학습을 했었다. 그런데 세상은 이렇게 단순하지 않다. 우리는 살면서 수많은 data를 생성한다. 기업이나 연구에서도 단순히 일회성 취미로 data 분석을 할 것이 아니라면 항상 갱신을 해줘야 한다. 이때, 시간이 지날수록 새로운 데이터가 생겨나게 된다. 이러한 상황에서 거울 나라의 앨리스의 붉은 여왕이 말한 것과 같이 우리는 기존의 데이터로만 가지고 분석을 한다면 저 멀리 뒤쳐지게 될 것이다. 그렇기에 우리는 새로운 데이터를 추가해 성능 향상을 위해 노력을 해야 한다. 그러면 데이터를 추가하려면 어떻게 해야 할까? 

우리가 한 가지 생각해볼 수 있는 방법은 지금까지 해온 것과 같이 기존의 data를 기반으로 새로운 data를 추가하여 전체 model을 학습하는 것이다. 그런데, 이렇게 전체 data를 포함해서 처음부터 다시 한다면 시간이 지날수록 data를 저장하는 공간이 늘어날 것이고, 학습에 드는 시간도 엄청나게 길어질 것이다. 더 많은 공간과 시간은 모두 비용이다. 대체로 이러한 비용은 적을수록 좋기에 우리는 이것을 줄이는 방법을 생각해볼 것이다. 

새로운 data를 추가하면서 data를 저장하는 공간과 학습 시간을 기존과 비슷하게 유지라고 싶다면, 결국 기존의 data를 버리는 수밖에 없다. 하지만 기존의 data를 버리면서 굳이 만든 새 model이 기존의 model보다 성능이 낮으면 어떻게 해야 할까? 굳이 시간과 비용을 들여서 새 model을 만들었는데 성능이 더 떨어지면 그건 그것 나름대로 억울하지 않을 수 없다.

따라서 우린 다시 조금 더 새로운 방법을 생각해볼 것이다. 결국 우리가 새로운 걸 하는데, 억울하지 않기 위해서는 훈련한 모델을 버리지 않아야 한다. 그런데 우리는 새로운 데이터에는 적응을 해야한다. 이러한 것을 위해서 여러 천재적인 개발자 분들께서는 점진적 학습 기법을 고안해내셨다. 여기서 우리가 배울 것은 그 중 대표적인 알고리즘인 확률적 경사 하강법(Stachastic Gradient Descent)이다. 

### 경사 하강법
아마 처음 stachastic gradient descent을 들었을 때 아마 이것이 무엇을 하는 것인지 이해하기 어려울 것이다. 이미 확률이라는 것만 들어도 머리가 아픈데, 경사에 하강? 이건 당췌 무슨 말인지 모르겠고 알고 싶지도 않을 수도 있다. 하지만 우리는 머신 러닝을 하기로 한 이상 이것을 알아야 한다. 먼저, 확률이라는 것은 넘어가고 경사, 하강부터 보자. 우선, 간단하게 예시를 들어보도록 하자. 예를 들어 아래의 그래프에서 최솟값을 구한다고 해보자. 어떻게 구해야 할까? 그냥 지금처럼 그래프를 다 그려서 눈으로 어디가 최솟값인지 확인해야 할까?

![image](https://www.dropbox.com/scl/fi/nnjb7ica04bsoq2k1rxx4/gradient_descent_raw.jpg?rlkey=8lldwy2b794adwzsu6pnwrltv&st=j2ff3an0&raw=1)

우리는 여기서 경사 하강법이라는 방법을 사용하는 것이다. 아래의 그래프처럼 경사를 내려가면서 최솟값을 찾는 방법이다. 아래의 그래프를 보면 일정한 간격으로 점이 그래프를 내려감을 확인할 수 있다. 그래서 결국에는 최솟값을 찾게 된다. 그런데 이게 뭐 어쨌다는 것인지 묻고 싶을 수도 있다. 조금씩 내려가서 가장 낮은 곳에 도달하는 것은 당연한 거 아닌가? 

![image](https://www.dropbox.com/scl/fi/kxpdrbxmmkpvv0cjn77sz/gradient_descent.jpg?rlkey=hw5m6fabjz5tca5y0ehlxj4f2&st=ukvx3tf5&raw=1)

이제 아래의 또 다른 그래프를 보자. 이것도 위 그래프와 동일하게 일정한 간격으로 점이 그래프를 따라 내려가지만, 조금 더 적은 횟수에, 그리고 최솟값에 도달하지는 못했다는 것을 알 수 있다.

![image](https://www.dropbox.com/scl/fi/mbbqisekqc7o8anw0ycgf/gradient_descent_rough.jpg?rlkey=b5ig1uscq7uurt1wylmrsd6ga&st=bjdisokh&raw=1)

이렇게 했을 때 또 의문이 생길 수 있다. 그래프에서 점이 내려가는 건 우리가 눈으로 봐서 알 수 있는데, 이걸 컴퓨터는 어떻게 이해할까? 컴퓨터도 눈으로 봐서 가장 낮은 것인지 알 수 있을까? 아쉽게도 그건 아니다. 만약 그게 가능했다면 이런 것도 다 필요가 없을 것이다. 그래서 우리는 고등학교 때 배운 미분이라는 것을 해볼 것이다. 아마 어떤 함수를 미분했을 때 0이 나오는 지점에서 우리는 극값을 구할 수 있다고 배웠을 것이다. 우리도 그것을 활용하는 것이다. 아래의 그래프처럼 원래 함수의 도함수를 구해서 0이 되는 부분을 찾으면 되는 것이다.

![iamge](https://www.dropbox.com/scl/fi/7g3nvch03kcqcuffxb7ha/gradient_descent_derivative.jpg?rlkey=hpifjzk2xnwejzctuhia4uzgt&st=lcvq34ig&raw=1)

그런데, 여기서 한 가지 문제가 발생할 수도 있다. 아래의 그래프처럼 단순히 아래로 볼록인 형태가 아니라 복잡한 형태의 그래프라면 어떻게 할까? 방금까지 한 방법대로라면 국소 최소값(Local Minimum)에 들어가 학습이 될 수밖에 없다. 하지만 우리가 원하는 것은 전역 최솟값(Global Miniumum)이다. 이러한 경우에는 어떻게 해야할까? 그래프를 그리고 첫번째 최솟값은 무시하라고 할까?
![image](https://www.dropbox.com/scl/fi/0thnsypy2hyefb4ck3jyf/gradient_descent_2.jpg?rlkey=f5eqh8udz8wf29liopoq13a6e&st=k456hwy6&raw=1)

여기에도 또 다른 방법이 하나 있다. 처음에는 크게 내려가고, 나중에는 세밀하게 조절하는 것이다. 이렇게 학습하면 작은 노이즈의 영향을 최소화하고, 최솟값을 구할 때는 조금 더 정확하게 구할 수 있게 된다.
![image](https://www.dropbox.com/scl/fi/2ied9r8akozzxa87lj4ig/gradient_descent_2_rough.jpg?rlkey=im6ph62ke0ewtr4he7kx4jppx&st=7fazucbi&raw=1)

이렇게 지금까지 무려 그래프를 6개나 그려가며 설명을 했다. 경사하강법은 대충 이런 식으로 흘러간다고 생각을 하면 될 것이다. 물론 앞에서 말한 것대로 해도 반드시 최솟값에 도달하게 될 것이라는 보장은 없다. 다만, 아무것도 안 하는 것보다는 성능이 좋을 것이라는 것은 분명하다. 이제 확률적이라는 것을 이해해보자. 

### 확률적 경사 하강법