---
title: "[혼공머신] 6주차 심층 신경망+신경망 모델 훈련"
date: 2025-02-24 01:38:01 +0900
categories: [대외활동, 혼공학습단]
tags: [extracurricular, hongong, machinelearning, python]
use_math: true
---
> 본 게시글은 한빛미디어의 혼자 공부하는 머신러닝+딥러닝을 바탕으로 작성되었습니다.
{: .prompt-info }

### 2개의 층
이전에 인공신경망 모델에서 층을 더 추가한다면 어떻게 될까? 기존에는 입력층과 출력층이 있었다. 그러나 이 사이에 층을 더 추가하게 된다면 이 층은 은닉층이 된다. 은닉층에는 활성화 함수가 포함이 되어 있는데, 활성화 함수는 층의 선형방정식의 계산값에 적용하는 함수이다. 이전에 살펴보았던 소프트맥스 함수, 시그모이드 함수와 같은 것이 있다.

은닉흥에 왜 활성화 함수를 사용할까? 그것은 간단하게 생각해볼 수 있다. 만약 추가한 활성화 함수가 단순히 선형적인 산술 연산만 한다면 실질적으로는 아무런 역할을 한 것이 없다. 하지만 이 활성화 함수는 비선형적으로 어떤 과정을 거쳐서 변환시켜주는 것이다. 예를 들어 시그모이드 함수의 경우, 출력값을 0과 1 사이로 제한해훈다. 

인공신경망은 층을 계속 추가해 입력 데이터에 대한 연속적인 학습이 가능하며, 이것이 매우 강력한 강점이다. 계속해서 층을 추가해가며 더욱 복잡한 모델을 만들고, 모델의 성능을 향상시킬 수 있는 것이다.

### 렐루 함수
딥러닝의 활성화 함수는 초기에는 시그모이드 함수를 많이 사용하였다. 그러나 이러한 함수는 좌우 끝부분의 경사가 완만하여 올바른 출력을 만드는 데 신속하게 대응할 수가 없다. 따라서 우리는 렐루 함수를 사용하여 입력이 양수일 때는 입력을 통과시키고 음수일 때는 0으로 만든다.

### 옵티마이저
지금까지 딥러닝이나 머신러닝 같은 경우, 여러 하이퍼 파라미터가 있었다. 그런데, 딥러닝에서는 까먹을 수도 있는 것이 있다. 은닉층의 개수, 은닉층의 뉴런 개수와 같은 것도 하이퍼 파라미터이다. 이것도 다 우리가 정해주는 것이지 모델이 학습해서 정하는 것이 아니기 때문이다.

케라스는 기본적으로 우리가 이전에 들어본 미니배치 경사하강법을 사용하며, 이것이 RMSprop이다. 그런데, 당연히 이것만 있지는 않다. 이러한 것을 옵티마이저라고 부르는데, 또 다른 예시로는 SGD가 있다. 미니배치 경사하강법이 있으면 당연히 우리가 같이 배운 확률적 경사하강법도 존재할 것이다. 이 SGD가 확률적 경사하강법이다.

그런데, 이러한 옵티마이저의 모멘텀 매개변수의 기본 값은 0이다. 이것을 만약 0보다 크게 한다면 이전의 그레디언트를 가속도처럼 사용하는 모멘텀 최적화를 사용한다. 일반적으로 0.9 이상으로 지정하게 된다.

그리고 네스테로프 모멘텀 최적화라는 것도 존재한다. 이것을 모멘텀 최적화를 2번 반복해 구현하는 것이다. 또한, 모델이 최적화에 가까이 갈수록 학습률을 낮출 수 있는데, 이것을 적응적 학습률이라고 하며 매개변수 튜닝을 적게 할 수 있게 해준다.

### 드롭 아웃
드롭이웃은 훈련 과정에서 일부 뉴런을 랜덤하게 0으로 만들어 과대적합을 막는 방법이다. 특정 뉴런에 과대하게 의존하는 것을 줄일 수 있게 해 안정적인 예측을 가능하게 해준다.

### 콜백
콜백의 경우, 훈련 과정에서 어떤 작업을 수행하게 해주는 것이다. 그리고 이를 활용하여 모델이 과대적합이 되기 전에 훈련을 중단하는 조기종료 기능을 사용할 수 있다. 이는 훈련 횟수를 제한하는 역할이나 과대적합을 막기 때문에 규제의 일종이라고 생각할 수도 있다.