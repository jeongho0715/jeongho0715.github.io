---
title: "[혼만딥] 2주차 LeNet 사용해보기"
date: 2025-07-16 01:38:01 +0900
categories: [대외활동, 혼공학습단]
tags: [extracurricular, hongong, deeplearning, python, lenet]
use_math: true
published: true
---
> 본 게시글은 한빛미디어의 혼자 만들면서 공부하는 딥러닝을 바탕으로 작성되었습니다.
{: .prompt-info }

## 활성화 함수

이번에는 지난번에 LeNet-5에 대한 이론적인 내용을 살펴보았으므로, 이번에는 LeNet-5를 실제 코드로 구현하고 얼마나 classification을 잘 하는지 확인해 볼 것이다. 

그런데, 그 전에 지난번에 까먹고 안 다뤘던 것이 하나 있다.


여기서 자주 보이는 sigmoid에 대해서 살펴보기로 하자. 우선, sigmoid의 식은 다음과 같다.

$$\sigma(X)=\frac{1}{1+e^{-X}}$$

그리고 위 식에 대한 plot을 그려보면 아래와 같다. 이건 LaTeX로 그린 것이기에 이렇게 이쁘게 나오는 것이다. 이걸 책에 나온 것처럼 한번 python으로도 그려보기로 하자.

<div style="display: flex; justify-content: center; flex-wrap: wrap;">
  <img src="https://www.dropbox.com/scl/fi/nlhz1oezqv3y579qzobbu/2.png?rlkey=97ktv8f4zcp45h49kpaokhmb0&st=32mp3lyr&raw=1" width="300" alt="sigmoid_plot"/>
</div>

Plot은 항상 우리가 사용하는 matplotlib를 활용해서 그릴 수 있다. 아까 살펴본 Sigmoid의 식을 그대로 python에 옮겨서 코드를 실행해보면 아래와 같이 위에서 그린 plot과 거의 동일한 모양인 것을 확인할 수 있다. 하지만 아까의 plot과 같이 정말 교재에 나오는 것 같은 깔끔한 그림은 나오지 않는다. 그렇기에 번거롭지만 LaTeX로 plot을 그리게 되어버렸다. LaTeX 최고!

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-10, 10, 0.2)

plt.plot(x, 1 / (1 + np.exp(-x)))
plt.title("Sigmoid Function")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.show()
```

<div style="display: flex; justify-content: center; flex-wrap: wrap;">
  <img src="https://www.dropbox.com/scl/fi/dbdcx48yhokxndrz7dqsd/4.png?rlkey=w77hlalz5dbc3g40jt0trs4dy&st=xv7jxyhi&raw=1" width="300" alt="sigmoid_plot_python1"/>
</div>

그런데, 여기서 한 가지 궁금증이 생길 수 있다. 분명히 책에서는 함수 구현을 조금 다르게 한 거 같은데 그건 다른 것일까? 사실 동일한 것이다. 다른 점은 sigmoid를 라이브러리에서 미리 정의해서 편하게 사용할 수 있게 설정해놓은 것이다. 그런데, 약간 다른 점은 SciPy에서 구현을 할 때, 최적화를 진행하여 sigmoid를 활용할 때 조금이나마 더 빠른 결과물을 확인할 수 있게 된다는 것이다. 그런데 지금과 같은 경우에는 뭘 쓰던 크게 상관 없기는 하다.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import expit

x = np.arange(-10, 10, 0.2)

plt.plot(x, expit(x))
plt.title("Sigmoid Function")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.show()
```

<div style="display: flex; justify-content: center; flex-wrap: wrap;">
  <img src="https://www.dropbox.com/scl/fi/qyat651xfs4gali8xdso8/5.png?rlkey=zgavz2fo8bsoz8m8o79gop70b&st=hotcsq4k&raw=1" width="300" alt="sigmoid_plot_python2"/>
</div>

그리고 이번에는 최근 많이 사용하는 ReLU를 살펴보기로 하자. ReLU는 생각보다 간단한 함수로, 0 이하인 경우는 그냥 0으로 설정해버리고 1 이상이면 그냥 그 값을 출력하는 함수이다.

$$\rm{ReLU}\it(X)=\begin{cases}\rm{max}\it(0, X), X\geq0\\0, X< 0\end{cases}$$

<div style="display: flex; justify-content: center; flex-wrap: wrap;">
  <img src="https://www.dropbox.com/scl/fi/yz2wb918bixm7wrivvbi0/1.png?rlkey=cpwk0fdut69qtv6lzisz9zcbt&st=9visc7m8&raw=1" width="300" alt="relu_plot"/>
</div>

이것도 python을 통해서 매우 간단하게 구현할 수 있다. 하나는 위 수식을 그대로 구현하는 방식으로 numpy의 maximum()을 쓰는 것이다.

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-10, 10, 0.2)

plt.plot(x, np.maximum(0, x))
plt.title("ReLU Function")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.show()
```

<div style="display: flex; justify-content: center; flex-wrap: wrap;">
  <img src="https://www.dropbox.com/scl/fi/mux4kvh6pc9r7efe936mh/6.png?rlkey=cfrc8y2ie5d6aub27ngfdnoao&st=b42vvcm8&raw=1" width="300" alt="relu_plot_python1"/>
</div>

또 다른 방법은 numpy의 clip을 사용하는 방식이다. 둘 다 동일하게 numpy 라이브러리의 기능을 사용하는 것이고, 동일한 출력을 내므로 어떤 것을 사용해도 크게 상관은 없다. 그런데 수식을 조금 더 명확하게 보여주는 maximum()을 쓰는 것이 여기서는 조금 더 낫지 않을까 생각이 된다.

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-10, 10, 0.2)

plt.plot(x, np.clip(x, 0, None))
plt.title("ReLU Function")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.show()
```

<div style="display: flex; justify-content: center; flex-wrap: wrap;">
  <img src="https://www.dropbox.com/scl/fi/bzcy5te2qtw3xr08tyvyb/7.png?rlkey=s7ylr8s3g2rkt1wzogrm9mi1m&st=puozsj79&raw=1" width="300" alt="relu_plot_python2"/>
</div>


먼저, 교재에서 제시하고 있는 LeNet-5 model 구현 코드는 다음과 같다. 지금은 엄청 간단하게 sequential model을 구현하는 것이기에 layer를 하나하나 추가하는 방식으로 model을 구현하고 있다. 이러한 방식 말고도 여러 구현 방식이 있으나 지금은 약간 귀찮기에 그냥 여기서 하는 그대로 해보기로 했다.

```python
import keras
from keras import layers, models

lenet5 = keras.Sequential()
lenet5.add(layers.Input(shape=(28, 28, 1)))
lenet5.add(layers.Conv2D(filters=6, kernel_size=5, activation='sigmoid', padding='same'))
lenet5.add(layers.AveragePooling2D(pool_size=2))
lenet5.add(layers.Conv2D(filters=16, kernel_size=5, activation='sigmoid'))
lenet5.add(layers.AveragePooling2D(pool_size=2))
lenet5.add(layers.Flatten())
lenet5.add(layers.Dense(120, activation='sigmoid'))
lenet5.add(layers.Dense(84, activation='sigmoid'))
lenet5.add(layers.Dense(10, activation='softmax'))
```

우선 논문에서는 activation function으로 sigmoid가 아니라 hyperbolic tangent를 사용했다고 했다. 이전의 sigmoid와 유사하지만 $\rm tanh(\it{X})$의

<div style="display: flex; justify-content: center; flex-wrap: wrap;">
  <img src="https://www.dropbox.com/scl/fi/ppxtmnzv36gn5nno18typ/3.png?rlkey=tmppf6cpn496q3lydhtx4t46s&st=an8feboc&raw=1" width="300" alt="tanh_plot"/>
</div>

```python
import tensorflow as tf
from tensorflow.keras import layers, models

def build_lenet_tf():
    model = models.Sequential([
        layers.Input(shape=(32, 32, 1)),
        layers.Conv2D(6, kernel_size=5, activation='tanh', padding='same'),
        layers.AveragePooling2D(pool_size=2, strides=2),
        layers.Conv2D(16, kernel_size=5, activation='tanh'),
        layers.AveragePooling2D(pool_size=2, strides=2),
        layers.Flatten(),
        layers.Dense(120, activation='tanh'),
        layers.Dense(84, activation='tanh'),
        layers.Dense(10, activation='softmax')
    ])
    return model

model_tf = build_lenet_tf()
model_tf.summary()
```

사실 여기서 더 추가로 들어가보자면, 단순한 hyperbolic tangent가 아니라 scaled hyperbolic tangent를 사용했다고 언급하고 있다. 식은 다음과 같고 동일하게 파이썬으로도 구현할 수 있다. 그런데 조금 귀찮으니 대충 hyperbolic tangent만 사용해서 실험해보기로 하자.

$$f(a)=1.7159\cdot\tanh(Sa)$$

```python
def scaled_tanh(x):
    return 1.7159*tf.math.tanh(2*x/3)
```

## 분류 실험
### 교재 기본 코드
제일 먼저 교재에 나온 그대로 실험을 한 번 해보기로 하자. 아까 살펴본 것과 같이 $28\times28$ 크기의 이미지를 입력받고, activation function을 sigmoid로 한 것이다.

```python
import keras
from keras import layers, models

lenet5 = keras.Sequential()
lenet5.add(layers.Input(shape=(28, 28, 1)))
lenet5.add(layers.Conv2D(filters=6, kernel_size=5, activation='sigmoid', padding='same'))
lenet5.add(layers.AveragePooling2D(pool_size=2))
lenet5.add(layers.Conv2D(filters=16, kernel_size=5, activation='sigmoid'))
lenet5.add(layers.AveragePooling2D(pool_size=2))
lenet5.add(layers.Flatten())
lenet5.add(layers.Dense(120, activation='sigmoid'))
lenet5.add(layers.Dense(84, activation='sigmoid'))
lenet5.add(layers.Dense(10, activation='softmax'))

lenet5.summary()
```

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>

<monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ conv2d (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">6</span>)      │           <span style="color: #00af00; text-decoration-color: #00af00">156</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ average_pooling2d               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">6</span>)      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">AveragePooling2D</span>)              │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)     │         <span style="color: #00af00; text-decoration-color: #00af00">2,416</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ average_pooling2d_1             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)       │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">AveragePooling2D</span>)              │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">400</span>)            │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">120</span>)            │        <span style="color: #00af00; text-decoration-color: #00af00">48,120</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">84</span>)             │        <span style="color: #00af00; text-decoration-color: #00af00">10,164</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>)             │           <span style="color: #00af00; text-decoration-color: #00af00">850</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>

다음으로 교재에 나온 것과 같이 fashion MNIST 데이터를 불러오고, 정규화를 진행해준 후 train data, validation data를 나눠준다. 이때, validation data는 20% 비율로 설정해준다.

```python
from tensorflow.keras.datasets import fashion_mnist
from sklearn.model_selection import train_test_split

(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
X_train = X_train.reshape(-1, 28, 28, 1)/255.0
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
```

그리고 학습 과정에서의 hyper-parameter들을 설정해준다. 여기서는 교재와 조금 다르게 patience를 5로 설정해서 조금 더 오래 성능 하락이 있더라도 학습을 진행하도록 하였다.
```python
checkpoint_cb = keras.callbacks.ModelCheckpoint('lenet5-model.keras', save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)
lenet5.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])
```

그리고 학습을 진행해준다. 교재에서는 적은 epoch를 설정했으나 여기서는 조금 더 큰 50으로 설정해줬다.
```python
hist = lenet5.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[checkpoint_cb, early_stopping_cb])
```

```
Epoch 1/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 2ms/step - accuracy: 0.4151 - loss: 1.5564 - val_accuracy: 0.7107 - val_loss: 0.7286<br>
Epoch 2/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.7351 - loss: 0.6802 - val_accuracy: 0.7424 - val_loss: 0.6459<br>
Epoch 3/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.7676 - loss: 0.5988 - val_accuracy: 0.7872 - val_loss: 0.5440<br>
Epoch 4/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8007 - loss: 0.5252 - val_accuracy: 0.8128 - val_loss: 0.5106<br>
Epoch 5/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8223 - loss: 0.4787 - val_accuracy: 0.8207 - val_loss: 0.4758<br>
Epoch 6/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8340 - loss: 0.4497 - val_accuracy: 0.8295 - val_loss: 0.4479<br>
Epoch 7/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8417 - loss: 0.4263 - val_accuracy: 0.8413 - val_loss: 0.4281<br>
Epoch 8/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8504 - loss: 0.4057 - val_accuracy: 0.8562 - val_loss: 0.3962<br>
Epoch 9/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8543 - loss: 0.3907 - val_accuracy: 0.8547 - val_loss: 0.3933<br>
Epoch 10/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8606 - loss: 0.3720 - val_accuracy: 0.8617 - val_loss: 0.3743<br>
Epoch 11/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8666 - loss: 0.3584 - val_accuracy: 0.8549 - val_loss: 0.3877<br>
Epoch 12/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8698 - loss: 0.3468 - val_accuracy: 0.8578 - val_loss: 0.3716<br>
Epoch 13/50<br>
...<br>
Epoch 43/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.9146 - loss: 0.2245 - val_accuracy: 0.8967 - val_loss: 0.3103<br>
Epoch 44/50<br>
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.9180 - loss: 0.2190 - val_accuracy: 0.9001 - val_loss: 0.3052<br>
```


이렇게 해서 최종적으로 최적의 모델에 대한 train accuracy는 약 91%, validation accuracy는 90%로 overfitting의 우려는 크게 없어 보인다.

```python
train_loss, train_acc = hist.history["loss"][-1], hist.history["accuracy"][-1]
val_loss, val_acc = hist.history["val_loss"][-1], hist.history["val_accuracy"][-1]

print(f"Train loss: {train_loss:.4f} | accuracy: {train_acc:.4%}")
print(f"Validation loss: {val_loss:.4f} | accuracy: {val_acc:.4%}")
```

```
Train loss: 0.2235 | accuracy: 91.6458%
Validation loss: 0.3052 | accuracy: 90.0083%
```

이제 이걸 plot을 그려서 어떻게 loss와 accuracy가 epoch에 따라서 변화했는지 살펴보자.

```python
import matplotlib.pyplot as plt

epochs = range(1, len(hist.history["loss"])+1)

plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.plot(epochs, hist.history["loss"], label="Train")
plt.plot(epochs, hist.history["val_loss"], label="Val")
plt.title("Loss per Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.subplot(1,2,2)
plt.plot(epochs, hist.history["accuracy"], label="Train")
plt.plot(epochs, hist.history["val_accuracy"], label="Val")
plt.title("Accuracy per Epoch")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

plt.tight_layout()
plt.show()
```

<div style="display: flex; justify-content: center; flex-wrap: wrap;">
  <img src="https://www.dropbox.com/scl/fi/mkkoq8fg2l7mqlcqyr1x9/8.png?rlkey=u0mhav23u0qjnja3420fpowrt&st=gh8hl1k9&raw=1" width="" alt="tanh_plot"/>
</div>



## 기본 과제
### LeNet으로 Fashion MNIST 분류 실습 후 에측 결과 화면 캡쳐하기

## 추가 과제
### 예측이 틀린 이미지를 골라 "왜 틀렸을까" 추측해 보기


## 참고문헌








