---
title: "최소제곱추정량 추정"
date: 2025-01-18 03:12:08 +0900
categories: [통계학, 회귀분석]
tags: [statistics, regression_analysis]
use_math: true
---

### 정의  
$Y$: 종속변수(Dependent Variable)  
$X$: 독립변수(Independent Variable)  
$\beta_0$: 절편(Intercept)  
$\beta_1$: 계수(Coefficient)  
$\varepsilon_i$: 오차(Error)  

### 기본 가정  
1. 선형성 가정: 변수 $X$와 $Y$ 사이에는 선형 관계가 존재한다.
2. 오차의 정규성: 오차는 정규분포를 따른다.
3. 오차의 평균: 오차의 평균은 0이다.
4. 오차의 분산: 오차는 동일한 분산 $\sigma^2$를 갖는다.
5. 오차의 독립성: 오차는 서로 독립적이다.
6. 독립변수의 고정: 독립변수 $X$는 확률변수가 아닌 고정된 값이다.
7. 독립변수의 오차: 독립변수는 오차 없이 측정된 것으로 가정한다.
8. 동식변수의 다중공선성: 독립변수는 서로 선형종속이 아닌, 독립적이다. 다중공선성이 존재하지 않다.
4. 종속변수의 신뢰성: 종속변수 $Y$는 동일하게 신뢰할 만하며, 결과 도출 과정에서 동등한 역할을 한다.

### 회귀분석의 목적
단순선형회귀 모형(Simple Linear Regression Model)  
$Y=\beta_0+\beta_1X+\varepsilon_i \tag{1}$

오차를 최소화하는 $\beta_0$, $\beta_1$를 구하는 것이 목표

만약 $x_1, x_2, \cdots, x_n$과 $y_1, y_2, \cdots, y_n$이 있을 때, (1)에 이것을 적용하면,   
$y_i=\beta_0+\beta_1x_i+\varepsilon_i \tag{2}$

(2)의 식을 $\varepsilon_i$에 대한 식으로 변형하면,  
$\varepsilon_i=y_i-\beta_0-\beta_1x_i \tag{3}$

이때, (3)의 오차가 최소화되어야 하는데, 오차를 그냥 모두 더하면 음수와 양수가 함께 있어 서로 상쇄될 수 있기에 제곱한 후에 더함.
$\sum^n_{i=1}\varepsilon_i^2=\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2 \tag{4}$

(4)를 $SSE(\beta_0, \beta_1)$(Sum of Squared Errors)로 정의.
$SSE(\beta_0, \beta_1)=\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2 \tag{5}$

$SSE(\beta_0, \beta_1)$를 최소화하는 $\beta_0, \beta_1$를 구하기 위해서는 $SSE(\beta_0, \beta_1)$를 $\beta_0, \beta_1$로 미분했을 때 0이 나오는 $\beta_0, \beta_1$를 구하면 됨.

### $\hat{\beta}_0$의 유도
$\frac{\partial SSE(\beta_0, \beta_1)}{\partial \beta_0}=0 \tag{6}$

(6)에 (5)를 대입하면,
$\frac{\partial}{\partial \beta_0}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2=0 \tag{8}$

Chain rule을 활용하면 $[f(x) \circ g(x)]'=f'[g(x)]g'(x)$이기에,  
만약, $f(\beta_0)=\beta_0^2, g(\beta_0)=y_i-\beta_0-\beta_1x_i$라고 한다면,  

$[f(\beta_0)]'=[\beta_0^2]'=2\beta_0 \tag{9}$

$[g(\beta_0)]'=[y_i-\beta_0-\beta_1x_i]'=-1 \tag{10}$

(9), (10)을 (8)에 대입하면,

$\frac{\partial}{\partial \beta_0}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2=-2\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i) \tag{11}$

(6)에 (11)를 대입하면,

$-2\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)=0 \tag{12}$

(12)의 양변에 -2를 나누면,

$\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)=0 \tag{13}$

$\sum^n_{i=1}y_i-\sum^n_{i=1}\beta_0-\sum^n_{i=1}\beta_1x_i=0 \tag{14}$

(14)의 $\beta_0$에는 $i$가 없으므로, 
$\sum^n_{i=1}y_i-n\beta_0-\beta_1\sum^n_{i=1}x_i=0 \tag{15}$

(15)의 양변에 $\frac{1}{n}$을 곱하면,
$\frac{1}{n}\sum^n_{i=1}y_i-\frac{1}{n}n\beta_0-\beta_1\frac{1}{n}\sum^n_{i=1}x_i=0 \tag{16}$

$\bar{y}-\beta_0-\beta_1\bar{x}=0 \tag{17}$

(16)을 $\bar{y}$에 대한 식으로 정리하면,
$\beta_0=\bar{y}-\beta_1\bar{x} \tag{18}$

### $\hat{\beta}_1$의 유도
이제 $SSE(\beta_0, \beta_1)$를 $\beta_1$로 미분하면,
$\frac{\partial SSE(\beta_0, \beta_1)}{\partial \beta_1}=0 \tag{19}$

$\beta_0$를 구했던 위의 과정과 같이 진행하면,

$\frac{\partial}{\partial \beta_1}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2=-2\sum^n_{i=1}x_i(y_i-\beta_0-\beta_1x_i) \tag{20}$

$\sum^n_{i=1}x_i(y_i-\beta_0-\beta_1x_i)=0 \tag{21}$

$\sum^n_{i=1}x_iy_i-\sum^n_{i=1}\beta_0x_i-\sum^n_{i=1}\beta_1x_i^2=0 \tag{22}$

$\sum^n_{i=1}x_iy_i-\beta_0\sum^n_{i=1}x_i-\beta_1\sum^n_{i=1}x_i^2=0 \tag{23}$

(18)을 (23)에 대입하면,

$\sum^n_{i=1}x_iy_i-(\bar{y}-\beta_1\bar{x})\sum^n_{i=1}x_i-\beta_1\sum^n_{i=1}x_i^2=0 \tag{24}$

$\sum^n_{i=1}x_iy_i-\bar{y}\sum^n_{i=1}x_i+\beta_1\bar{x}\sum^n_{i=1}x_i-\beta_1\sum^n_{i=1}x_i^2=0 \tag{25}$

$\sum^n_{i=1}x_iy_i-\sum^n_{i=1}x_i\bar{y}=\beta_1\sum^n_{i=1}x_i^2-\beta_1\sum^n_{i=1}x_i\bar{x} \tag{26}$

$\sum^n_{i=1}x_i(y_i-\bar{y})=\beta_1\sum^n_{i=1}x_i(x_i-\bar{x}) \tag{27}$

$\beta_1=\frac{\sum^n_{i=1}x_i(y_i-\bar{y})}{\sum^n_{i=1}x_i(x_i-\bar{x})} \tag{28}$

계산의 편의성을 위해 $S_{XX}, S_{YY}, S_{XY}$를 정의하면,  
$S_{XX}=\sum^n_{i=1}(x_i-\bar{x})(x_i-\bar{x}) \tag{29}$  
$S_{YY}=\sum^n_{i=1}(y_i-\bar{y})(y_i-\bar{y}) \tag{30}$  
$S_{XY}=\sum^n_{i=1}(x_i-\bar{x})(y_i-\bar{y}) \tag{31}$  

(28)의 분자를 살펴보면,

$\sum^n_{i=1}x_i(y_i-\bar{y})=\sum^n_{i=1}x_i(y_i-\bar{y})-\sum^n_{i=1}\bar{x}(y_i-\bar{y})+\sum^n_{i=1}\bar{x}(y_i-\bar{y}) \tag{32}$  
$\sum^n_{i=1}x_i(y_i-\bar{y})-\sum^n_{i=1}\bar{x}(y_i-\bar{y})+\sum^n_{i=1}\bar{x}(y_i-\bar{y})=\sum^n_{i=1}(x_i-\bar{x})(y_i-\bar{y})+\sum^n_{i=1}\bar{x}(y_i-\bar{y}) \tag{33}$  

$\because \sum^n_{i=1}(y_i-\bar{y})=\sum^n_{i=1}y_i-\sum^n_{i=1}\bar{y} \tag{34}$
$\sum^n_{i=1}y_i-\sum^n_{i=1}\bar{y}=\sum^n_{i=1}y_i-n\bar{y}=\frac{1}{n}n\sum^n_{i=1}y_i-n\bar{y} \tag{35}$
$\frac{1}{n}n\sum^n_{i=1}y_i-n\bar{y}=n\bar{y}-n\bar{y}=0 \tag{36}$

(36)을 (33)에 대입하면,
$\sum^n_{i=1}(x_i-\bar{x})(y_i-\bar{y})+\sum^n_{i=1}\bar{x}(y_i-\bar{y})=\sum^n_{i=1}(x_i-\bar{x})(y_i-\bar{y}) \tag{37}$  

(37)과 마찬가지로 (28)의 분모를 살펴보면,
$\sum^n_{i=1}x_i(x_i-\bar{x})=\sum^n_{i=1}x_i(x_i-\bar{x})-\sum^n_{i=1}\bar{x}(x_i-\bar{x})+\sum^n_{i=1}\bar{x}(x_i-\bar{x}) \tag{38}$  
$\sum^n_{i=1}x_i(x_i-\bar{x})-\sum^n_{i=1}\bar{x}(x_i-\bar{x})+\sum^n_{i=1}\bar{x}(x_i-\bar{x})=\sum^n_{i=1}(x_i-\bar{x})(x_i-\bar{x})+\sum^n_{i=1}\bar{x}(x_i-\bar{x}) \tag{39}$  

$\because \sum^n_{i=1}(x_i-\bar{x})=\sum^n_{i=1}x_i-\sum^n_{i=1}\bar{x} \tag{40}$
$\sum^n_{i=1}x_i-\sum^n_{i=1}\bar{x}=\sum^n_{i=1}x_i-n\bar{x}=\frac{1}{n}n\sum^n_{i=1}x_i-n\bar{x} \tag{41}$
$\frac{1}{n}n\sum^n_{i=1}x_i-n\bar{x}=n\bar{x}-n\bar{x}=0 \tag{42}$

(42)를 (39)에 대입하면,
$\sum^n_{i=1}(x_i-\bar{x})(x_i-\bar{x})+\sum^n_{i=1}\bar{x}(x_i-\bar{x})=\sum^n_{i=1}(x_i-\bar{x})(x_i-\bar{x})=\sum^n_{i=1}(x_i-\bar{x})^2 \tag{43}$  

(37), (43)을 (28)에 대입하면,
$\beta_1=\frac{\sum^n_{i=1}x_i(y_i-\bar{y})}{\sum^n_{i=1}x_i(x_i-\bar{x})}=\frac{\sum^n_{i=1}(x_i-\bar{x})(y_i-\bar{y})}{\sum^n_{i=1}(x_i-\bar{x})^2} \tag{44}$

(29), (31)로 (44)를 표현하면,
$\frac{\sum^n_{i=1}(x_i-\bar{x})(y_i-\bar{y})}{\sum^n_{i=1}(x_i-\bar{x})^2}=\frac{S_{XY}}{S_{XX}} \tag{45}$

### 정규방정식
따라서 $SSE(\beta_0, \beta_1)$를 최소화하는 $\beta_0, \beta_1$를 추정했을 때, (18), (45)에 의해, 다음 정규방정식이 만들어짐. ■
$\hat{\beta}_ 0=\bar{y}-\hat{\beta}_ 1\bar{x} \tag{46}$  
$\hat{\beta}_ 1=\frac{S_ {XY}}{S_ {XX}} \tag{47}$