---
title: "[혼공머신] 3주차 04-2 확률적 경사 하강법"
date: 2025-01-27 12:26:09 +0900
categories: [대외활동, 혼공학습단]
tags: [extracurricular, hongong, machinelearning, python]
use_math: true
---
> 본 게시글은 한빛미디어의 혼자 공부하는 머신러닝+딥러닝을 바탕으로 작성되었습니다.
{: .prompt-info }

### 점진적 학습
우리는 지금까지 한정된 data를 가지고 학습을 했었다. 그런데 세상은 이렇게 단순하지 않다. 우리는 살면서 수많은 data를 생성한다. 기업이나 연구에서도 단순히 일회성 취미로 data 분석을 할 것이 아니라면 항상 갱신을 해줘야 한다. 이때, 시간이 지날수록 새로운 데이터가 생겨나게 된다. 이러한 상황에서 거울 나라의 앨리스의 붉은 여왕이 말한 것과 같이 우리는 기존의 데이터로만 가지고 분석을 한다면 저 멀리 뒤쳐지게 될 것이다. 그렇기에 우리는 새로운 데이터를 추가해 성능 향상을 위해 노력을 해야 한다. 그러면 데이터를 추가하려면 어떻게 해야 할까? 

우리가 한 가지 생각해볼 수 있는 방법은 지금까지 해온 것과 같이 기존의 data를 기반으로 새로운 data를 추가하여 전체 model을 학습하는 것이다. 그런데, 이렇게 전체 data를 포함해서 처음부터 다시 한다면 시간이 지날수록 data를 저장하는 공간이 늘어날 것이고, 학습에 드는 시간도 엄청나게 길어질 것이다. 더 많은 공간과 시간은 모두 비용이다. 대체로 이러한 비용은 적을수록 좋기에 우리는 이것을 줄이는 방법을 생각해볼 것이다. 

새로운 data를 추가하면서 data를 저장하는 공간과 학습 시간을 기존과 비슷하게 유지라고 싶다면, 결국 기존의 data를 버리는 수밖에 없다. 하지만 기존의 data를 버리면서 굳이 만든 새 model이 기존의 model보다 성능이 낮으면 어떻게 해야 할까? 굳이 시간과 비용을 들여서 새 model을 만들었는데 성능이 더 떨어지면 그건 그것 나름대로 억울하지 않을 수 없다.

따라서 우린 다시 조금 더 새로운 방법을 생각해볼 것이다. 결국 우리가 새로운 걸 하는데, 억울하지 않기 위해서는 훈련한 모델을 버리지 않아야 한다. 그런데 우리는 새로운 데이터에는 적응을 해야한다. 이러한 것을 위해서 여러 천재적인 개발자 분들께서는 점진적 학습 기법을 고안해내셨다. 여기서 우리가 배울 것은 그 중 대표적인 알고리즘인 확률적 경사 하강법(Stachastic Gradient Descent)이다. 

### 확률적 경사 하강법
아마 처음 stachastic gradient descent을 들었을 때 아마 이것이 무엇을 하는 것인지 이해하기 어려울 것이다. 이미 확률이라는 것만 들어도 머리가 아픈데, 경사에 하강? 이건 당췌 무슨 말인지 모르겠고 알고 싶지도 않을 수도 있다. 하지만 우리는 머신 러닝을 하기로 한 이상 이것을 알아야 한다. 먼저, 확률이라는 것은 넘어가고 경사, 하강부터 보자. 우선, 간단하게 예시를 들어보도록 하자. 예를 들어 아래의 그래프에서 최솟값을 구한다고 해보자. 어떻게 구해야 할까? 그냥 지금처럼 그래프를 다 그려서 눈으로 어디가 최솟값인지 확인해야 할까?

![image](https://www.dropbox.com/scl/fi/nnjb7ica04bsoq2k1rxx4/gradient_descent_raw.jpg?rlkey=8lldwy2b794adwzsu6pnwrltv&st=j2ff3an0&raw=1)

우리는 여기서 경사 하강법이라는 방법을 사용하는 것이다. 아래의 그래프처럼 

![image](https://www.dropbox.com/scl/fi/kxpdrbxmmkpvv0cjn77sz/gradient_descent.jpg?rlkey=hw5m6fabjz5tca5y0ehlxj4f2&st=ukvx3tf5&raw=1)

![iamge](https://www.dropbox.com/scl/fi/7g3nvch03kcqcuffxb7ha/gradient_descent_derivative.jpg?rlkey=hpifjzk2xnwejzctuhia4uzgt&st=lcvq34ig&dl=0)

![image](https://www.dropbox.com/scl/fi/mbbqisekqc7o8anw0ycgf/gradient_descent_rough.jpg?rlkey=b5ig1uscq7uurt1wylmrsd6ga&st=bjdisokh&raw=1)

