---
title: "[논문 리뷰] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
date: 2025-07-17 09:55:08 +0900
categories: [논문 리뷰, 딥러닝]
tags: [paper_review, deep_learning, language_model, rag, nlp]
use_math: true
published: false
---
> 본 게시글은 Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks를 바탕으로 작성되었습니다.
{: .prompt-info }

## 초록
기존의 pre-trained llm은 parameter에 정보를 저장

- 하위 NLP task에서 fine-tuning 시 SOTA

한계: 파라미터에 정보를 저장

- 저장된 정보 접근 및 정밀 조작 제한
- 결정 근거 제공 및 정보 업데이트 제한
- 전문 지식 필요한 task에서는 해당 분야 특화 모델에 비해 성능 낮음

따라서 본 연구에서는 사전학습된 parametric 메모리와 non-parametric 메모리를 결합해 텍스트 생성 모델 RAG 제안
- Parametric 메모리: pre-trained Seq2Seq 모델
- Non-parametric 메모리: 위키피디아 dense vector index
- Pre-trained 신경망 기반 리트리버로 접근

두 가지 RAG 구조 비교
- 전체 sequence에 대해 동일한 검색 문서를 사용
- Token마다 서로 다른 검색 문서를 사용

다양한 전문 지식이 필요한 NLP 작업에 대해 fine-tuning 후 평가
3가지 개방형 질의응답 task에서 SOTA 달성

## 서론
사전학습된 산경망 언어 모델은 외부 메모리 접근 없이 파라미터화된 암묵적 지식을 활용해 깊이있는 지식을 학습할 수 있음.
단점
- 메모리를 확장 수정하기 어려움
- 예측 결과 근거 제공 어려움
- 환각 증상 가능

따라서 parametric 메모리와 검색 기반 non-parametric 메모리를 결합한 하이브리드 모델로 해결
지식을 직접적으로 수정하거나 확장 가능하고, 접근된 지식 검토 및 해석이 가능

최근 제안된 모델 REALM, ORQA가 있는데, 마스킹된 언어 모델과 미분 가능한 리트리버를 결합했음
=> 오픈 도메인 추출형 질문 응답에만 적용함

여기서는 seq2seq 모델로도 확장함

RAG 구조 설계
- Parametric 메모리: pre-trained Seq2Seq Transformer
- Non-parametric 메모리: 위키피디아 dense vector index, pre-trained 신경망 리트리버로 접근

하나의 확률 모델로 결합해 end-to-end 방식으로 학습

리트리버는 Dense Passage Retriever 사용
입력에는 조건화된 잠재 문서(latent document) 제공
Seq2Seq 모델(BART)는 잠재 문서와 입력을 동시에 조건으로 해 출력 생성

잠재 문서는 Top-k 방식으로 marginalize됨, 두 가지 방식으로 동작 가능
- 출력 전체 기반: 모든 토큰이 동일한 문서에 의해 생성 가능
- 토큰 단위 기반: 각 토큰이 서로 다른 문서에 의해 생성 가능

T5나 BART와 마찬가지로 RAG는 모든 Seq2Seq 작업에 fine-tuning 가능
생성기와 리트리버는 공동으로 학습

이전 non-parametric 메모리 구조를 처음부터 학습시키는 아키텍쳐 제안된 바 있음
- Memory networks, Stack-augmented network, memory layers

그러나 여기는 차이점 있음
- parametric 메모리와 non-parametric 메모리 모두 pre-trained됨
- 광범위한 지식으로 사전 로드되어 있음
- 사전학습된 접근 메커니즘으로 추가적 학습 없이도 지식 접근이 가능함

실험 결과, parametric 메모리, non-parametric 메모리를 생성과 결합하는 것이 외부 지식 소스 없이 인간이 해결하기 어려운 전문 분야 task에 유리함

RAG는 다음 오픈 도메인 QA 데이터셋에서 SOTA 달성
- Natural Questions
- WebQuestions
- CuratedTrec

또한, TriviaQA에서 추출형 방법을 썼던 최근 방법을 크게 능가. 제약 없는 생성으로 더 우수한 결과 생성

전분 분야 언어 생성에서는 MS-MARCO, Jeopardy 질문 생성을 함

RAG는 BART 기반 모델보다 더 사실적이고 구체적이고, 다양한 응답 생성
FEVER 사실검증 작업에서는 강한 리트리버 슈퍼비전을 사용한 최신 파이프라인 모델에 비해 얼마 차이 안 나 높은 성능 기록

non-parametric 메모리 교체를 통해 모델 지식을 업데이트할 수 있음도 확인

## 방법
입력 시퀀스 $x$를 사용해서 텍스트 문서 $z$를 검색하고, 이를 추가적인 문맥 정보로 사용해 목표 시퀀스 $y$ 생성

1. 리트리버 $p_{\theta}(z|x)$
사용자의 질문 $x$가 주어졌을 때, 텍스트 문서 $z$에 대한 상위 $k$개로 잘린 확률분포를 반환

2. 생성기 $p_{\phi}(y_i|x, z, y_{1:i=1})$
파라미터 $\phi$를 가지고 처음부터 지금까지 생성된 $i=1$개의 토큰 $y_{1:i-1}$과 원래의 입력 $x$, 그리고 검색된 텍스트 문서 $z$를 바탕으로 현재 토큰 $y_i$를 생성함

리트리버와 생성기를 end-to-end로 학습하기 위해서 검색된 문서를 latent variable로 간주, 즉, 잠재문서로 간주

이 잠재 문서에 대해 marginalize하는 서로 다른 두 가지 모델 제안, 이를 통해 생성 텍스트에 대한 확률 분포를 만들어냄

- 첫번째 방식 RAG-Sequence: 하나의 동일한 문서를 사용해 모든 출력 토큰 예측
- 두번째 방식: RAG-Token: 각 출력 토큰마다 서로 다른 문서를 기반으로 예측

### 2.1 모델
#### RAG-Sequence 모델
- 동일한 문서 하나를 사용해서 전체 출력 시퀀스 생성
- 검색된 문서를 단일 잠재 변수로 간주하고 이를 marginalize해 Seq2Seq 확률 $p(y|x)$를 Top-k 근사를 통해 계산

리트리버를 통해 상위 K개의 문서를 검색
생성기는 각 문서 $z$에 대해 출력 시퀀스 y에 대한 확률 $p(y|x, z)$를 계산
이 확률들은 각 문서에 대한 사후확률 $p_{\theta}(z|x)$로 가중합되어 marginalized됨

$$p_{RAG-Sequence}(y|x)\approx\sum_{z\in top-k(p(\cdot|x))}p_\eta(z|x)p_\theta(y|x, z)$$

$$p(y|x)=\sum_zp(y, z|x)=\sum_z p(z|x)\cdot p(y|x, z)$$
그러나 전체 문서 집합에 대해 합산하기 어렵기 때문에 top-K를 써서 근사함.

이때, $p_\theta(y|x, z)$는 각 토큰에 대해 생성된 확률의 곱이다. 따라서 각 검색된 문서 $z$에 대해서 전체 시퀀스의 생성 확률을 계산하고 이를 $p_\theta(z|x)$에 따라 가중평균하여 최종 출력 확률을 얻는 방식
$$p_\theta(y|x, z)=\prod_i p_\theta(y_i|x, z, y_{1:i-1})$$

$$=\sum_{z\in top-k(p(\cdot|x))}p_\eta(z|x)\prod^N_i p_\theta(y_i|x, z, y_{1:i-1})$$

$n=2$라면
$$p(A_1, A_2)=p(A_1)\cdot p(A_2|A_1)$$
$$\because p(A_2|A_1)=\frac{p(A_1, A_2)}{p(A_1)}, p(A_1)>0$$

$n=3$이라면
$$p(A_1, A_2, A_3)=p((A_1, A_2), A_3)=p(A_1, A_2)\cdot p(A_3|A_1, A_2)$$
$$\because p(A_1, A_2)=p(A_1)\cdot p(A_2|A_1)$$
$$p(A_1, A_2, A_3)=p(A_1)\cdot p(A_2|A_1)\cdot p(A_3|A_1, A_2)$$

$n=k$일 때 다음과 같다고 가정하면
$$p(A_1, \cdots, A_k)=\prod^k_{i=1}p(A_i|A_1, \cdots, A_{i-1})$$

$n=k+1$일 때
$$p(A_1, \cdots, A_{k+1})=p(A_1, \cdots, A_k)\cdot p(A_{k+1}|A_1, \cdots, A_k)$$

이때, 다음과 같이 가정했기에
$$\because p(A_1, \cdots, A_k)=\prod^k_{i=1}p(A_i|A_1, \cdots, A_{i-1})$$

$$p(A_1, \cdots, A_{k+1})=\prod^k_{i=1}p(A_i|A_1, \cdots, A_{i-1})\cdot p(A_{k+1}|A_1, \cdots, A_k)=\prod^{k+1}_{i=1}p(A_i|A_1, \cdots, A_{i-1})$$

따라서 모든 $n\geq2$에 대해 성립

구하려고 하는 것은
$$p(y_1, y_2, \cdots, y_N|x, z)$$

조건부 연쇄 법칙으로 다음과 같이 분해 가능
$$p(y_1, y_2, \cdots, y_N|x, z)=\prod^N_{i=1}p(y_i|x, z, y_{1:i-1})$$

$N=1$일 때, 첫번쨰 토큰은 이전 토큰이 없음
$$p(y_1|x, z)$$

따라서 아래와 같이 작동한다고 볼 수 있음, 이때, <BOS>는 문장의 시작을 의미하는 special token
$$p(y_1|x, z)=p(y_1|x, z, <BOS>)$$

$N=2$일 때, 첫번째 토큰이 존재하기에 이것도 함께 사용함
$$p(y_1, y_2|x, z)=p(y_1|x, z)\cdot p(y_2|x, z, y_1)$$

<br><br><br><br><br>

만약 N에 대하여 $\mathbf{y}=(y_1, y_2, \cdots, y_n)$일 때,
$$p(\mathbf{y}|x, z)=p(y_1|x, z)\cdot p(y_2|x, z, y_1)\cdots p(y_N|x, z, y_1, \cdots, y_{N-1})$$
$$p(\mathbf{y}|x, z)=\prod^N_{i=1}p(y_i|x, z, y_{1:i-1})$$

지금까지의 모든 토큰들을 활용하여 다음 토큰을 예측하는 것임








#### RAG-Token 모델
각 목표 토큰에 대해 서로 다른 잠재 문서를 선택 후 이에 따라 marginalize함
따라서 생성기는 정답을 생성할 때 여러 문서로부터 콘텐츠를 선택
리트리버를 통해 상위 K개의 문서를 검색한 뒤 생성기는 각 문서마다 다음 토큰에 대한 확률분포를 계산

이후 이 확률분포를 marginalize한 후, 다음 토큰에 대해서도 같은 과정을 반복
$$p_{RAG-Token}(y|x)\approx \prod^N_i\sum_{z\in top-k(p(\cdot|x))}p_\eta(z|x)p_\theta(y_i|x, z, y_{1:i-1})$$

목표 시퀀스가 길이 1인 시퀀스로 간주되는 시퀀스 분류 작업의 경우
RAG-Seq 모델과 RAG-Token 모델이 동일하게 작동

기존 auto regressive 모델은
$$p(\mathbf{y}|x, z)=\prod^N_{i=1}p(y_i|x, z, y_{1:i-1})$$
로 하나의 문서를 본 것이 아니라 
각 토큰 $y_i$마다 잠재문서 $z$를 고려한 확률의 기대값을 취함

하나의 문서만 보고 생성하지 않고, 여러 문서의 정보로부터 평균화된 토큰 $y_i$를 생성

$$p(y_i|x, y_{1:i-1})=\sum_zp(z|x)\cdotp(y_i|x, z, y_{1:i-1})$$
