---
title: "[논문 리뷰] Tune My Adam, Please!"
date: 2025-09-05 23:43:36 +0900
categories: [논문 리뷰, 딥러닝]
tags: [deeplearning, hyperparameter, optimization]
use_math: true
published: true
---

# 논문 정리
## 서론
- Hyperparameter Optimization(HPO)은 많은 시간과 비용 필요함.
- Bayesian Optimization(BO)은 효율적이나 여전히 모델 전체 학습이 필요함.
- Freeze-thaw BO은 모델 중 좋을 것 같은 후보만을 계속 학습시키는 방식.
- FT-BO는 실제 모델을 전부 학습하지 않고 일부만 학습한 후, 대리 모델을 사용해서 성능을 예측하는데 실질적으로 hyperparameter가 학습에 어떤 영향을 주는지는 고려하지 않음.
- 한계 극복 위해 PFN을 기반으로 하는 단일 대리 모델인 FT-PFN을 사용하는 ifBO 제안하였으나, 결국 단일 모델만을 사용하기에 다양한 상황에 사용하기엔 한계 존재함.
- 따라서 새로운 대리 모델 Adam-PFN 제안

## 데이터셋
- TaskSet
- Adam을 최적화된 1,162개의 학습 곡선 포함됨.
- 각 task에는 1,000개의 무작위 hyperparameter 설정에 대한 학습 곡선이 포함됨.
- Log scale에서 균등분포로 샘플링된 learning rate, $\beta_1, \beta_2, \epsilon$, 그리고 L1, L2 정규화 parameter, 학습률 감소 스케줄을 제어하는 두 개의 parameter 포함됨.

## 학습곡선 증강 기법
- Beta 분포의 CDF를 사용하는 국소적 학습 곡선 증강 기법인 CDF-augment 사용함.
- Beta 분포의 최빈값 $\mu$를 $[0, 1]$ 구간에서, 집중도 $\kappa$를 $[2, 5]$ 구간에서 균등 분포로 샘플링하여 CDF를 구함.
- 학습 곡선 $y$를 CDF를 이용해 forward-propagate함.
- 빠르게 올라가거나 느리게 올라가는 등 다양한 난이도에서의 학습곡선 변형 생성 가능.

$$y'=F_{Beta}(y;\mu,\kappa)$$

## 대리 모델 학습
- Adam optimizer의 실제 학습 곡선에서 증강된 데이터를 기반으로 하는 PFN임.
- FT-PFN과 파이프라인이 동일하나, 학습 곡선과 hyperparameter 설정을 합성 사전분포로부터 샘플링하지 않고, CDF-augment를 사용한다는 차이 있음.
- 878개의 과제를 선택해 과제마다 1,000개의 서로 다른 hyperparameter 설정을 적용해 학습곡선을 얻음.
- 이렇게 얻은 학습 곡선에서 증강된 학습곡선들을 추가로 얻고 이미 관찰된 곡선 부분과 아직 모르는 곡선의 뒷부분을 나눔.

## 실험
- 학습에 사용되지 않은 12개 과제를 사용해 새로운 과제에서 얼마나 잘 학습 곡선을 예측하고 HPO를 하는지 평가함.
- 기존 대리 모델인 HyHPO, DPL, FT-PFN, Uniform Predictor과 Adam-PFN 비교함.
- ifBO 프레임워크에 FT-PFN 대신 Adam-PFN을 넣어 성능을 확인함.
- HyperBand, ASHA, GP 기반 Freeze-thaw, Random Search로 HPO 성능 비교함.
- 추가로, CDF-augment의 효과를 보기 위해 또 다른 기법인 Mixup 학습 곡선 증강을 한 것과 증강을 안 한 것을 비교함.

## 결과
- Adam-PFN을 활용한 경우, Log-likelihood와 MSE 모두에서 다른 기법을 능가함.
- PFN 기반 기법은 사전에 학습되기에 추론 시간이 짧음.
- 얼마나 빨리 적절한 hyperparameter를 찾았는지 보여주는 normalized regret을 봤을 때에도 Adam-PFN이 더 빠르게 높은 성능에 도달함.
- TaskSet과 별개인 Pytorch Examples에서 시험해본 결과 초기 단계에서는 Adam-PFN이 가장 우수했으나 장기적인 탐색 과정에서는 FT-PFN의 성능이 높았음.

## 향후 연구
- HPO 특화 및 사전 지식을 사용하는 
- 탐색 공간과 하이퍼파라미터의 수가 제한되어 있다는 한계 존재.
- 학습 및 평가에 사용되지 않은 hyperparameter를 기본값으로 사용해볼 수 있을 것.
- 새로운 hyperparameter 증강 기법을 사용하기.
- 초기에는 Adam-PFN로 탐색하고 이후에는 FT-PFN로 탐색하거나 둘을 섞는 방법은 어떨까?

# 참고문헌
Athanasiadis, T., Adriaensen, S., Müller, S., & Hutter, F. (2025). Tune My Adam, Please! (No. arXiv:2508.19733). arXiv. https://doi.org/10.48550/arXiv.2508.19733
