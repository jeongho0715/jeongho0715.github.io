---
title: "[혼공머신] 2주차 03-1 k-최근접 이웃 회귀"
date: 2025-01-15 12:53:09 +0900
categories: [대외활동, 혼공학습단]
tags: [extracurricular, hongong, machinelearning, python]
use_math: true
---

### KNN Classification
이제 지금까지 살펴본 KNN 알고리즘을 조금 더 살펴볼 것이다. 지난번에 했던 것은 KNN 알고리즘을 이용하여 도미와 빙어의 classification였다. 그런데 그냥 classification 말고 예측도 가능할까?

아마 통계학을 공부했다면 질리도록 배울 것인 회귀분석(Regression Analysis)에 대해서 알 것이다. 몰라도 상관은 없다. 회귀를 간단하게 말한다면 도미와 빙어 중 하나로 data point를 classify하는 것이 아니라, 일부 도미의 무게 또는 길이가 없을 때 그 없는 data를 predict하는 것이다. KNN 알고리즘은 이 두 가지를 모두 할 수 있다. 이 두 가지를 설명하기에 앞서, 간략하게 설명한 적이 있지만, 이번엔 KNN 알고리즘 자체에 대해서 다루기 때문에 KNN 알고리즘에 대해서 처음부터 설명을 해보겠다.

우선, 쉽게 이해하기 위해 classification 문제를 해결하는 방법을 예를 들어 설명하겠다. 아래와 같이 점들이 있다고 할 때, 파란색 점이랑 붉은색 점, 두 가지 class가 있다고 가정하겠다. 이때 우리는 주황색 점이 둘 중 어느 것에 속할 확률이 더 큰 지, 어느 것에 더 가까운지 확인하고 싶다.

![image](https://www.dropbox.com/scl/fi/gxsoh3j5u5zdita73f325/2501150001-1.jpg?rlkey=1e01rf1ueuq5rlws8c47606lt&st=1f869jbr&raw=1)

이러한 경우, 우리의 KNN 알고리즘은 매우 분명한 답을 내어준다. 가장 가까운 $K$개의 점을 찾고, 그 $K$개의 점들 중에서 제일 많은 개수를 가진 class를 찾아 그 class에 주황색 점을 배정하는 것이다.

이걸 시각적으로 간단하게 표현해보자. 주황색 점을 중심으로 하는 원이 있을 때, $K=1$일 때 원 안에는 붉은색 점 하나뿐이므로 주황색 점은 붉은색 점의 class에 속할 확률이 높다고 할 수 있다.

![image](https://www.dropbox.com/scl/fi/z47b5h9g3wz3pc9z1905p/2501150001-2.jpg?rlkey=hflw1qf5swdaoh8koxdukv0e3&st=ign87z86&raw=1)

만약 $K=3$이라면 아래와 같이 표현할 수 있다. 아까 $K=1$이었을 때는 붉은색 점이 하나 있고, 파란색 점이 0개 있어서 붉은색 점의 class로 분류를 하였으나, 이번에는 아까와 동일하게 붉은색 점이 하나 있으나, 파란색 점이 2개로, 파란색 점이 더 많다. 따라서 이번에는 주황색 점이 파란색 점의 class로 배정이 될 확률이 높게 나타난다고 할 수 있다.

![image](https://www.dropbox.com/scl/fi/2kap0k0o5uzkmjb1m4mkx/2501150001-3.jpg?rlkey=3w0ahtqiux9y35y2c4fyl0x1m&st=bbghwx1z&raw=1)

지금까지 본 것과 같이 KNN 알고리즘은 거리를 바탕으로 한다는 것을 알 수 있다. 방금 우리가 원으로 살펴본 것은 원의 반지름을 활용한 것으로, 이를 조금 더 작관적으로 선으로 그어서 표현하면 아래와 같이 표현할 수 있다.

![image](https://www.dropbox.com/scl/fi/l8ybjnwlo31vo2rftn50t/2501150001-4.jpg?rlkey=mwzdg3kay9xmt9klaso1ui8ia&st=k6apvdrf&raw=1)

이렇게 점과 점 사이의 거리를 바탕으로 우리는 KNN 알고리즘을 적용하게 된다. 이것을 통해서 classification 문제를 해결하는 방법은 알게 되었다. 그런데 regression 문제는 어떻게 해결할까?

### KNN Regression
사실 이것도 간단하다. 약간 그래프가 달라진 것 같지만, 그래프를 이쁘게 그리기 위해 바꾸었다고 생각해주고 아래 그래프를 보자. 아까와 같이 $K=3$이라고 하면 숫자를 예쁘게 적은 3개의 점만 선택이 될 것이다. 그럼 이 점들의 평균을 구하면 된다. $\frac{3+4+6}{3}\approx4.3$으로 약 4.3이 됨을 알 수 있다. 

![image](https://www.dropbox.com/scl/fi/mlof18xw7924wp9un4sf5/2501150001-5.jpg?rlkey=3wsoqbi783xdgny6b1wwyycfn&st=jdtogutp&raw=1)

그런데 우리는 그래프를 봤을 때 대충 점의 $y$값이 5 정도 일것이라고 생각할 수 있다. 하지만 이건 설명을 하기 쉽도록 하기 위함이다. 사실 잘 생각해보면 약간 이상하다. 분명 아까 KNN 알고리즘은 거리를 바탕으로 한다고 했는데 주황색 점의 $y$값을 추정하는 것이 아니었나? 그럼 주황색 점의 $y$값이 없으니 점과 점 사이의 거리를 구할 수 없는 게 아닐까? 라고 생각할 수 있다. 

물론 이러한 생각이 안 들어도 상관 없다. 살면서 이걸 손으로 풀어볼 일은 대학 수업에서 말고는 거의 없을 것이다. 그래도 이걸 잘 생각보면 다른 방법으로 해결할 수 있다는 것을 알 수 있다. 우리는 $y$값은 없는 대신 $x$값은 가지고 있다. 따라서 우리는 $x$값만을 가지고 점과 점 사이의 거리를 구하면 되는 것이다. 즉, $x$만을 기준으로 가장 가까운 3개의 점을 찾고, 그 3개의 점의 $y$값의 mean을 구하면 되는 것이다.

### 농어 무게 예측
지금까지 도미와 빙어를 너무 많이 살펴보았다. 이정도로 봤으면 비린내가 날 것이기에 한빛미디어는 이번엔 싱싱한 농어를 새로 가져왔다.

```python
import numpy as np

perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,
       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,
       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,
       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,
       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,
       44.0])
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])
```

Data가 있으면 우리는 당연히 visualization을 해야한다. 해보자.
```python
import matplotlib.pyplot as plt

plt.scatter(perch_length, perch_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![image](https://www.dropbox.com/scl/fi/002zlmcfborzh5h78a7pt/2501150001-6.jpg?rlkey=1svqfuufuilkv3h26355axb4w&st=fh4sqni3&raw=1)

이제 학습을 해야 하므로 우린 무의식적으로 train data와 test data로 나눠야 한다. 나는 친절하기 때문에 아래와 같이 코드를 다 적었다. 적어도 되지만, 기억이 안 나는 사람은 복사를 해도 된다.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(perch_length, perch_weight, random_state=0)
```

지난번에 scikit-learn에 대해서 공부했을 때, 데이터는 2차원 배열이어야 해서 numpy를 통해 변환을 했던 기억이 있을 것이다. 이제 그걸 해보자.

```python
X_train = X_train.reshape(-1, 1)
X_test = X_test.reshape(-1, 1)
```

### 결정계수
Scikit-learn에서 사용하기 적당하게 data를 변환했으므로 이제 학습을 해야 할 시간이다. 학습을 한 다음에 점수를 확인해보자.

```python
from sklearn.neighbors import KNeighborsRegressor

knn = KNeighborsRegressor()
knn.fit(X_train, y_train)
knn.score(X_test, y_test)
```
> 0.9162089041423581


### 과대적합, 과소적합
