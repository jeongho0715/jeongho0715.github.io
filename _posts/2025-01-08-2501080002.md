---
title: "[혼공머신] 1주차 02-2 데이터 전처리"
date: 2025-01-08 12:08:59 +0900
categories: [대외활동, 혼공학습단]
tags: [extracurricular, hongong, machinelearning, python]
use_math: true
---

우선, 지난번에 썼던 도미와 빙어 길이, 무게 data를 가져와보자. 이번에는 numpy를 적극적으로 활용해서 데이터를 정리해볼 것이다.

```python
import numpy as np

length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 
                10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 
                7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]

X = np.column_stack((length, weight))
y = np.concatenate((np.ones(35), np.zeros(14)))
```

이렇게 하면 지난번에 했던 것과 동일하게 input data와 target data를 생성할 수 있다. 교재의 말의 의하면 더욱 세련되게? 할 수 있는 방법이다. 개인적으로 pandas의 dataframe만 주구장창 쓰다 보니 numpy의 array에 조금 소홀했던 것 같다는 느낌이 약간 들었다. 나중에 시간이 되면 numpy만 따로 공부를 해보는 것도 괜찮을 것 같다는 생각이 들었다.

이제 지난 게시글에서 언급했던 train_test_split을 사용해볼 것이다. 우리의 위대한 scikit-learn은 training data와 test data를 나누기 위한 함수로 train_test_split를 제공한다. 이것은 데이터 분석을 하면서 계속 사용하게 될 것이기에 외워야 한다.
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
```

이렇게 함수를 설정하면 전체 데이터의 25%를 test data로 분리하고, 나머지 75%를 training_data로 분리한다. 이때, 랜덤으로 데이터를 뽑아주기는 하지만, 랜덤이란 것은 예전에 했던 것과 같이 전부 도미 데이터만 뽑힐 수도 있다는 것이다. 그렇기 때문에 이를 방지하기 위해서 특정 class의 비율이 일정하게 되도록 뽑으라고 아래와 같이 stratify를 통해 y에 대해서 일정 비율로 뽑히도록 설정을 할 수도 있다. 

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)
```