---
title: "[혼공머신] 6주차 심층 신경망+신경망 모델 훈련"
date: 2025-02-24 01:38:01 +0900
categories: [대외활동, 혼공학습단]
tags: [extracurricular, hongong, machinelearning, python]
use_math: true
published: false
---
> 본 게시글은 한빛미디어의 혼자 공부하는 머신러닝+딥러닝을 바탕으로 작성되었습니다.
{: .prompt-info }

### 2개의 층
이전에 인공신경망 모델에서 층을 더 추가한다면 어떻게 될까? 기존에는 입력층과 출력층이 있었다. 그러나 이 사이에 층을 더 추가하게 된다면 이 층은 은닉층이 된다. 은닉층에는 활성화 함수가 포함이 되어 있는데, 활성화 함수는 층의 선형방정식의 계산값에 적용하는 함수이다. 이전에 살펴보았던 소프트맥스 함수, 시그모이드 함수와 같은 것이 있다.

은닉흥에 왜 활성화 함수를 사용할까? 그것은 간단하게 생각해볼 수 있다. 만약 추가한 활성화 함수가 단순히 선형적인 산술 연산만 한다면 실질적으로는 아무런 역할을 한 것이 없다. 하지만 이 활성화 함수는 비선형적으로 어떤 과정을 거쳐서 변환시켜주는 것이다. 예를 들어 시그모이드 함수의 경우, 출력값을 0과 1 사이로 제한해훈다. 

인공신경망은 층을 계속 추가해 입력 데이터에 대한 연속적인 학습이 가능하며, 이것이 매우 강력한 강점이다. 계속해서 층을 추가해가며 더욱 복잡한 모델을 만들고, 모델의 성능을 향상시킬 수 있는 것이다.

### 렐루 함수
딥러닝의 활성화 함수는 초기에는 시그모이드 함수를 많이 사용하였다. 그러나 이러한 함수는 좌우 끝부분의 경사가 완만하여 올바른 출력을 만드는 데 신속하게 대응할 수가 없다. 따라서 우리는 렐루 함수를 사용하여 입력이 양수일 때는 입력을 통과시키고 음수일 때는 0으로 만든다.

### 옵티마이저
지금까지 딥러닝이나 머신러닝 같은 경우, 하이퍼파라미터를 사람이 모두 직접 입력해서 작성했었다.